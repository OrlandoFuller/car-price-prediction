{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sustained-norman",
   "metadata": {},
   "source": [
    "# 2. Machine learning for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-supervision",
   "metadata": {},
   "source": [
    "This chapter covers\n",
    "\n",
    "* Creating a car-price prediction project with a linear regression model\n",
    "* Doing an initial exploratory data analysis with Jupyter notebooks\n",
    "* Setting up a validation framework\n",
    "* Implementing the linear regression model from scratch\n",
    "* Performing simple feature engineering for the model\n",
    "* Keeping the model under control with regularization\n",
    "* Using the model to predict car prices\n",
    "\n",
    "In chapter 1, we talked about supervised machine learning, in which we teach machine\n",
    "learning models how to identify patterns in data by giving them examples.\n",
    "\n",
    "\n",
    "Suppose that we have a dataset with descriptions of cars, like make, model, and age, and\n",
    "we would like to use machine learning to predict their prices. These characteristics of cars are\n",
    "called features, and the price is the target variable - something we want to predict. Then the\n",
    "model gets the features and combines them to output the price.\n",
    "\n",
    "\n",
    "This is an example of supervised learning: we have some information about the price of\n",
    "some cars, and we can use it to predict the price of others. In chapter 1, we also talked about\n",
    "different types of supervised learning: regression and classification. When the target variable\n",
    "is numerical, we have a regression problem, and when the target variable is categorical, we\n",
    "have a classification problem.\n",
    "\n",
    "\n",
    "In this chapter, we will create a regression model, and we will start with the simplest one:\n",
    "linear regression. We will implement the algorithms ourselves, which is simple enough to do in\n",
    "a few lines of code. At the same time, it’s very illustrative, and it will teach you how to deal\n",
    "with NumPy arrays and perform basic matrix operations such as matrix multiplication and matrix inversion. We will also come across problems of numerical instability when inverting a\n",
    "matrix and see how regularization helps solve them. \n",
    "\n",
    "\n",
    "## 2.1 Car-price prediction project\n",
    "\n",
    "\n",
    "The problem we will solve in this chapter is predicting the price of a car.\n",
    "\n",
    "\n",
    "Suppose that we have a website where people can sell and buy used cars. When posting\n",
    "an ad on our website, the sellers often struggle to come up with a meaningful price. We want\n",
    "to help our users with automatic price recommendation. We ask the sellers to specify model,\n",
    "make, year, mileage, and other important characteristics of a car, and based on that\n",
    "information, we want to suggest the best price.\n",
    "\n",
    "\n",
    "One of the product managers in the company accidentally came across an open dataset\n",
    "with car prices and asked us to have a look at it. We checked the data and saw that it contains\n",
    "all the important features as well as the recommended price — exactly what we need for our\n",
    "use case. Thus, we decided to use this dataset for building the price recommendation\n",
    "algorithm.\n",
    "\n",
    "\n",
    "The plan for the project is the following:\n",
    "\n",
    "\n",
    "1. First, we download the dataset.\n",
    "\n",
    "2. Next, we do some preliminary analysis of the data.\n",
    "\n",
    "3. After that, we set up a validation strategy to make sure our model produces correct\n",
    "predictions.\n",
    "\n",
    "4. Then we implement a linear regression model in Python and NumPy.\n",
    "\n",
    "5. Next, we cover feature engineering - to extract important features from the data to\n",
    "improve the model\n",
    "\n",
    "6. Finally, we see how to make our model stable with regularization and use it to predict\n",
    "car prices. \n",
    "\n",
    "### 2.1.1 Downloading the dataset\n",
    "\n",
    "\n",
    "The first thing we do for this project is to install all the required libraries: Python, NumPy,\n",
    "Pandas, and Jupyter notebook. The easiest way to do it is to use a Python distribution called\n",
    "Anaconda (https://www.anaconda.com). Please refer to appendix A for installation guidelines.\n",
    "\n",
    "\n",
    "After the libraries are installed, we need to download the dataset. There are multiple\n",
    "options for doing this. You can download it manually through the kaggle web interface. It’s\n",
    "available at https://www.kaggle.com/CooperUnion/cardataset.\n",
    "\n",
    "Go there, open it, and click\n",
    "the download link. The other option is using the kaggle command-line interface (CLI), which is\n",
    "a tool for programmatic access to all datasets available via kaggle. For this chapter, we will\n",
    "use the second option. We describe how to configure the kaggle CLI in appendix A. \n",
    "\n",
    "\n",
    "**NOTE: Kaggle is an online community for people who are interested in machine learning. It is mostly known\n",
    "for hosting machine learning competitions, but it is also a data sharing platform where anyone can share a\n",
    "dataset. More than 16,000 datasets are available for anyone to use. It is a great source of project ideas and\n",
    "very useful for machine learning projects.**\n",
    "\n",
    "In this chapter as well as throughout the book, we will actively use NumPy. We cover all\n",
    "necessary NumPy operations as we go along, but please refer to appendix C for a more indepth introduction.\n",
    "\n",
    "\n",
    "The source code for this project is available in the book’s repository in github at\n",
    "https://github.com/alexeygrigorev/mlbookcamp-code in chapter-02-car-price.\n",
    "\n",
    "\n",
    "As the first step, we will create a folder for this project. We can give it any name, such as\n",
    "chapter-02-car-price:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "jewish-space",
   "metadata": {},
   "source": [
    "mkdir chapter-02-car-price\n",
    "cd chapter-02-car-price "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-adobe",
   "metadata": {},
   "source": [
    "Then we download the dataset:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "authentic-barcelona",
   "metadata": {},
   "source": [
    "kaggle datasets download -d CooperUnion/cardataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-paraguay",
   "metadata": {},
   "source": [
    "This command downloads the cardataset.zip file, which is a zip archive. Let’s unpack it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aging-perry",
   "metadata": {},
   "source": [
    "unzip cardataset.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-problem",
   "metadata": {},
   "source": [
    "Inside, there’s one file: data.csv.\n",
    "\n",
    "When we have the dataset, let’s do the next step: understanding it. \n",
    "\n",
    "## 2.2 Exploratory data analysis\n",
    "\n",
    "\n",
    "Understanding data is an important step in the machine learning process. Before we can train\n",
    "any model, we need to know what kind of data we have and whether it is useful. We do this\n",
    "with exploratory data analysis (EDA).\n",
    "\n",
    "\n",
    "We look at the dataset to learn:\n",
    "\n",
    "* The distribution of the target variable\n",
    "* The features in this dataset\n",
    "* The distribution of values in these features\n",
    "* The quality of the data\n",
    "* The number of missing values\n",
    "\n",
    "### 2.2.1 Exploratory data analysis toolbox\n",
    "\n",
    "\n",
    "The main tools for this analysis are Jupyter notebook, Matplotlib and Pandas:\n",
    "    \n",
    "* Jupyter notebook is a tool for interactive execution of Python code. It allows to execute\n",
    "a piece of code and immediately see the outcome. In addition to that we can display\n",
    "charts and add notes with comments in free text. It also supports other languages such\n",
    "as R or Julia (hence the name: Jupyter stands for Julia, Python, R), but we will only use\n",
    "it for Python.\n",
    "\n",
    "* Matplotlib is a library for plotting. It is very powerful and allows you to create different\n",
    "types of visualizations, such as line chars, bar charts, histograms and many more.\n",
    "\n",
    "* Pandas is a library for working with tabular data. It can read data from any source, be\n",
    "it a csv file, a json file or a database.\n",
    "We will also use Seaborn, another tool for plotting that is built on top of Matplotlib and makes\n",
    "it easier to draw charts.\n",
    "Let’s start a Jupyter notebook by executing the following command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wooden-samoa",
   "metadata": {},
   "source": [
    "jupyter notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-exhibition",
   "metadata": {},
   "source": [
    "**This command starts a Jupyter notebook server in the current directory and opens it in the\n",
    "default web browser (figure 2.1).**\n",
    "\n",
    "If Jupyter is running on a remote server, it requires additional configuration. Please refer to\n",
    "appendix A for details on the setup.\n",
    "\n",
    "\n",
    "Now let’s create a notebook for this project. Click New, then select Python 3 in the\n",
    "Notebooks section. We can call it chapter-02-car-price-project - click the current title\n",
    "(Untitled), and replace it with the new one.\n",
    "\n",
    "\n",
    "First, we need to import all the libraries required for this project. Write the following in the\n",
    "first cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "colonial-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy: a library for numerical operations\n",
    "import pandas as pd # A\n",
    "\n",
    "#  Import Pandas: a library for tabular data\n",
    "import numpy as np #B\n",
    "\n",
    "# Import plotting libraries: matplotlib and seaborn\n",
    "import seaborn as sns # C\n",
    "from matplotlib import pyplot as plt # C\n",
    "\n",
    "# Make sure that plots are rendered correctly in jupyter notebooks. \n",
    "# D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-quantity",
   "metadata": {},
   "source": [
    "The first two lines, A and B, are imports for required libraries: NumPy for numeric operations\n",
    "and Pandas for tabular data. The convention is to import these libraries using shorter aliases\n",
    "(such as pd in import pandas as pd). This convention is very common in the Python machine\n",
    "learning community, and everybody follows it.\n",
    "\n",
    "\n",
    "The next two lines, C, are imports for plotting libraries. The first one, matplotlib, is a\n",
    "library for creating good-quality visualizations. It’s not always easy to use this library as is.\n",
    "Some libraries make using Matplotlib simpler, and Seaborn is one of them.\n",
    "\n",
    "\n",
    "Finally, %matplotlib inline in D tells Jupyter to expect plots in the notebook, so it will be\n",
    "able to render them when we need them.\n",
    "\n",
    "\n",
    "Press Shift+Enter or click Run to execute the content of the selected cell.\n",
    "We will not get into more detail about Jupyter notebooks. Check the official website2 to\n",
    "learn more about it. The site has plenty of documentation and examples that will help you\n",
    "master it.\n",
    "\n",
    "\n",
    "### 2.2.2 Reading and preparing data\n",
    "\n",
    "\n",
    "Now let’s read our dataset. We can use the read_csv function from Pandas for that purpose.\n",
    "Put the following code in the next cell and again press Shift+Enter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "apparent-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-hormone",
   "metadata": {},
   "source": [
    "This line of code reads the csv file and writes the results to a variable named df, which is\n",
    "short for dataframe. Now we can check how many rows are there. Let’s use the len function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "computational-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-comedy",
   "metadata": {},
   "source": [
    "The function prints 11914, which means that there are almost 12,000 cars in this dataset\n",
    "(figure 2.2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-reservoir",
   "metadata": {},
   "source": [
    "**Figure 2.2. Jupyter notebooks are interactive. We can type some code in a cell, execute it, and see the results\n",
    "immediately, which is ideal for exploratory data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chubby-architect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Year</th>\n",
       "      <th>Engine Fuel Type</th>\n",
       "      <th>Engine HP</th>\n",
       "      <th>Engine Cylinders</th>\n",
       "      <th>Transmission Type</th>\n",
       "      <th>Driven_Wheels</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Market Category</th>\n",
       "      <th>Vehicle Size</th>\n",
       "      <th>Vehicle Style</th>\n",
       "      <th>highway MPG</th>\n",
       "      <th>city mpg</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>MSRP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMW</td>\n",
       "      <td>1 Series M</td>\n",
       "      <td>2011</td>\n",
       "      <td>premium unleaded (required)</td>\n",
       "      <td>335.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>rear wheel drive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Factory Tuner,Luxury,High-Performance</td>\n",
       "      <td>Compact</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>3916</td>\n",
       "      <td>46135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>1 Series</td>\n",
       "      <td>2011</td>\n",
       "      <td>premium unleaded (required)</td>\n",
       "      <td>300.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>MANUAL</td>\n",
       "      <td>rear wheel drive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Luxury,Performance</td>\n",
       "      <td>Compact</td>\n",
       "      <td>Convertible</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>3916</td>\n",
       "      <td>40650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Make       Model  Year             Engine Fuel Type  Engine HP  \\\n",
       "0  BMW  1 Series M  2011  premium unleaded (required)      335.0   \n",
       "1  BMW    1 Series  2011  premium unleaded (required)      300.0   \n",
       "\n",
       "   Engine Cylinders Transmission Type     Driven_Wheels  Number of Doors  \\\n",
       "0               6.0            MANUAL  rear wheel drive              2.0   \n",
       "1               6.0            MANUAL  rear wheel drive              2.0   \n",
       "\n",
       "                         Market Category Vehicle Size Vehicle Style  \\\n",
       "0  Factory Tuner,Luxury,High-Performance      Compact         Coupe   \n",
       "1                     Luxury,Performance      Compact   Convertible   \n",
       "\n",
       "   highway MPG  city mpg  Popularity   MSRP  \n",
       "0           26        19        3916  46135  \n",
       "1           28        19        3916  40650  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-niagara",
   "metadata": {},
   "source": [
    "**Figure 2.3 The output of the head() function of a Pandas dataframe: it shows the first five rows of the dataset.\n",
    "This output allows us to understand what the data looks like.**\n",
    "\n",
    "This gives us an idea of what the data looks like. We can already see that there are some\n",
    "inconsistencies in this dataset: the column names sometimes have spaces and sometimes\n",
    "have underscores (_). The same is true for feature values: sometimes they’re capitalized and\n",
    "sometimes they are short strings with spaces. This is inconvenient and confusing, but we can\n",
    "solve this by normalizing them: replace all spaces with underscores and lowercase all letters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "employed-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lowercase all the column names, and replace spaces with underscores\n",
    "df.columns = df.columns.str.lower().str.replace(' ','_') # A\n",
    "\n",
    "# Select only columns with string values. \n",
    "string_columns = list(df.dtypes[df.dtypes == 'object'].index) # B\n",
    "\n",
    "# Lowercase and replace spaces with underscores for values in all string columns of the dataframe. \n",
    "for col in string_columns:\n",
    "    df[col] = df[col].str.lower().str.replace(' ','_') # C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-mexico",
   "metadata": {},
   "source": [
    "In A and C, we use the special str attribute. Using it, we can apply string operations to the\n",
    "entire column at that same time without writing any for loops. We use it to lowercase the\n",
    "column names and the content of these columns as well as to replace spaces with\n",
    "underscores.\n",
    "We can use this attribute only for columns with string values inside. This is exactly why we\n",
    "first select such columns in B.\n",
    "\n",
    "**NOTE: In this chapter and subsequent chapters, we cover relevant Pandas operations as we go along, but at\n",
    "a fairly high level. Please refer to appendix D for a more consistent and in-depth introduction to Pandas.**\n",
    "\n",
    "After this initial preprocessing, the dataframe looks more uniform (figure 2.4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protecting-processing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>engine_fuel_type</th>\n",
       "      <th>engine_hp</th>\n",
       "      <th>engine_cylinders</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>driven_wheels</th>\n",
       "      <th>number_of_doors</th>\n",
       "      <th>market_category</th>\n",
       "      <th>vehicle_size</th>\n",
       "      <th>vehicle_style</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>popularity</th>\n",
       "      <th>msrp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bmw</td>\n",
       "      <td>1_series_m</td>\n",
       "      <td>2011</td>\n",
       "      <td>premium_unleaded_(required)</td>\n",
       "      <td>335.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>rear_wheel_drive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>factory_tuner,luxury,high-performance</td>\n",
       "      <td>compact</td>\n",
       "      <td>coupe</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>3916</td>\n",
       "      <td>46135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmw</td>\n",
       "      <td>1_series</td>\n",
       "      <td>2011</td>\n",
       "      <td>premium_unleaded_(required)</td>\n",
       "      <td>300.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>rear_wheel_drive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>luxury,performance</td>\n",
       "      <td>compact</td>\n",
       "      <td>convertible</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>3916</td>\n",
       "      <td>40650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  make       model  year             engine_fuel_type  engine_hp  \\\n",
       "0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0   \n",
       "1  bmw    1_series  2011  premium_unleaded_(required)      300.0   \n",
       "\n",
       "   engine_cylinders transmission_type     driven_wheels  number_of_doors  \\\n",
       "0               6.0            manual  rear_wheel_drive              2.0   \n",
       "1               6.0            manual  rear_wheel_drive              2.0   \n",
       "\n",
       "                         market_category vehicle_size vehicle_style  \\\n",
       "0  factory_tuner,luxury,high-performance      compact         coupe   \n",
       "1                     luxury,performance      compact   convertible   \n",
       "\n",
       "   highway_mpg  city_mpg  popularity   msrp  \n",
       "0           26        19        3916  46135  \n",
       "1           28        19        3916  40650  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-columbus",
   "metadata": {},
   "source": [
    "**Figure 2.4: The result of preprocessing the data. The column names and values are normalized: they are\n",
    "lowercased, and the spaces are converted to underscores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-seafood",
   "metadata": {},
   "source": [
    "As we see, this dataset contains multiple columns:\n",
    "* make — make of a car (BMW, Toyota, and so on)\n",
    "* model — model of a car\n",
    "* year — year when the car was manufactured\n",
    "* engine_fuel_type — type of fuel the engine needs (diesel, electric, and so on)\n",
    "* engine_hp — horsepower of the engine\n",
    "* engine_cylinders — number of cylinders in the engine\n",
    "* transmission_type — type of transmission (automatic or manual)\n",
    "* driven_wheels — front, rear, all\n",
    "* number_of_doors — number of doors a car has\n",
    "* market_category — luxury, crossover, and so on\n",
    "* vehicle_size — compact, midsize, or large\n",
    "* vehicle_style — sedan or convertible \n",
    "* highway_mpg — miles per gallon (mpg) on the highway\n",
    "* city_mpg — miles per gallon in the city\n",
    "* popularity — number of times the car was mentioned in a Twitter stream\n",
    "* msrp — manufacturer’s suggested retail price\n",
    "\n",
    "For us, the most interesting column here is the last one: MSRP (manufacturer’s suggested\n",
    "retail price, or simply the price of a car). We will use this column for predicting the prices of a\n",
    "car.\n",
    "\n",
    "\n",
    "### 2.2.3 Target variable analysis\n",
    "\n",
    "\n",
    "The MSRP column contain the important information — it’s our target variable, the y, which is\n",
    "the value that we want to learn to predict.\n",
    "\n",
    "\n",
    "One of the first steps of exploratory data analysis should always be to look at what the\n",
    "values of y look like. We typically do this by checking the distribution of y: a visual description\n",
    "of what the possible values of y can be and how often they occur. This type of visualization is\n",
    "called a histogram.\n",
    "\n",
    "\n",
    "We will use Seaborn to plot the histogram, so type the following in the Jupyter notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "champion-entry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bizof\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='msrp'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.msrp, kde=False) \n",
    "# sns.displot(df.msrp, kde=False) \n",
    "# sns.histplot(df.msrp, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-contrary",
   "metadata": {},
   "source": [
    "After plotting this graph, we immediately notice that the distribution of prices has a very long\n",
    "tail. There are many cars with low prices on the left side, but the number quickly drops, and\n",
    "there’s a long tail of very few cars with high prices (see figure 2.5). \n",
    "\n",
    "**Figure 2.5 The distribution of the prices in the dataset. We see many values at the low end of the price axis and\n",
    "almost nothing at the high end. This is a long tail distribution, which is a typical situation for prices. There are\n",
    "many items with low prices and very few expensive ones.**\n",
    "\n",
    "We can have a closer look by zooming in a bit and looking at values below $100,000 (figure\n",
    "2.6): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collectible-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='msrp'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.msrp[df.msrp < 100000], kde=False)\n",
    "# sns.displot(df.msrp[df.msrp < 100000], kde=False)\n",
    "# sns.histplot(df.msrp[df.msrp < 100000], kde=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-munich",
   "metadata": {},
   "source": [
    "**Figure 2.6. The distribution of the prices for cars below $100,000. Looking only at car prices below $100,000\n",
    "allows us to see the head of the distribution better. We also notice a lot of cars that cost $1,000.**\n",
    "\n",
    "The long tail makes it quite difficult for us to see the distribution, but it has an even stronger\n",
    "effect on a model: such distribution can greatly confuse the model, so it won’t learn well\n",
    "enough. One way to solve this problem is log transformation. If we apply the log function to\n",
    "the prices, it removes the undesired effect (figure 2.7). \n",
    "\n",
    "**Figure 2.7 The logarithm of the price. The effect of the long tail is removed, and we can see the entire\n",
    "distribution in one plot.**\n",
    "\n",
    "\n",
    "The +1 part is important in cases that have zeros. The logarithm of zero is minus infinity, but\n",
    "the logarithm of one is zero. If our values are all non-negative, by adding 1, we make sure\n",
    "that the transformed values do not go below zero.\n",
    "\n",
    "\n",
    "For our specific case, zero values are not an issue. All the prices we have start at $1,000;\n",
    "but it’s still a convention that we follow. NumPy has a function that performs this\n",
    "transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "legendary-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_price = np.log1p(df.msrp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-piece",
   "metadata": {},
   "source": [
    "To look at the distribution of the prices after the transformation, we can use the same distplot\n",
    "function (figure 2.7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "broken-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='msrp'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUE0lEQVR4nO3df6zd9X3f8edrZqWQDAXqCzP+IbPITQqoXZIrQ38oQyIVVhZh1onNoVmslMxKRJekW5TgIRVNkyWiROmabSBZQHFUfsxK02GlS4BZbWmjAHEDBIzD8OrMdnDwXVnbrNlYDO/9cb7dDtfnXPvec+8513yeD8k63/P5fr7n+75w7+t8zuf746SqkCS14W9MugBJ0vgY+pLUEENfkhpi6EtSQwx9SWrIWZMu4FRWrlxZ69evn3QZknTGWLlyJQ899NBDVbVp9rplH/rr169n3759ky5Dks4oSVYOand6R5IacsrQT3J3kuNJnh2w7pNJqv8dJcn2JAeTPJ/kmr72dyV5plv3hSRZvB9DknQ6Tmekfw9w0rxQkrXALwKH+9ouBbYAl3Xb3J5kRbf6DmAbsKH7d9JrSpKW1ilDv6oeBV4esOo3gE8B/fdx2Aw8UFWvVNUh4CCwMckq4Lyq+kb17vvwReC6UYuXJM3Pgub0k1wLfK+qnp61ajVwpO/50a5tdbc8u33Y629Lsi/JvpmZmYWUKEkaYN6hn+Rc4Bbg1wetHtBWc7QPVFU7q2q6qqanpqbmW6IkaYiFnLL5VuAS4OnuWOwa4FtJNtIbwa/t67sGeLFrXzOgXZI0RvMe6VfVM1V1YVWtr6r19AL9nVX1fWAPsCXJ2UkuoXfA9omqOgb8IMmV3Vk7HwQeXLwfQ5J0Ok7nlM37gW8Ab0tyNMmNw/pW1X5gN/Ac8DXgpqp6tVv9UeBOegd3/yvw1RFrlyTNU5b7l6hMT0+XV+QK4L7HDw9sv+GKdWOuRFr+kvxJVU3PbveKXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhqykK9LlJbMsHvmS1ocjvQlqSGGviQ1xNCXpIY4p68znt+dK50+R/qS1BBDX5IacsrQT3J3kuNJnu1r+2yS7yT5dpLfTfKWvnXbkxxM8nySa/ra35XkmW7dF5Jk0X8aSdKcTmdO/x7g3wFf7Gt7BNheVSeSfAbYDnw6yaXAFuAy4GLgPyf5yap6FbgD2AY8BvwnYBPw1cX6QQZxrleSXu+UI/2qehR4eVbbw1V1onv6GLCmW94MPFBVr1TVIeAgsDHJKuC8qvpGVRW9N5DrFulnkCSdpsWY0/8V/v+IfTVwpG/d0a5tdbc8u32gJNuS7Euyb2ZmZhFKlCTBiKGf5BbgBHDvXzcN6FZztA9UVTurarqqpqempkYpUZLUZ8Hn6SfZCrwPuLqbsoHeCH5tX7c1wItd+5oB7ZKkMVrQSD/JJuDTwLVV9cO+VXuALUnOTnIJsAF4oqqOAT9IcmV31s4HgQdHrF2SNE+nHOknuR+4CliZ5ChwK72zdc4GHunOvHysqj5SVfuT7Aaeozftc1N35g7AR+mdCXQOvWMAS3rmjiTpZKcM/ap6/4Dmu+bovwPYMaB9H3D5vKqTJC0qr8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyIK/REVa7u57/PDA9huuWDfmSqTlw5G+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNOWXoJ7k7yfEkz/a1XZDkkSQvdI/n963bnuRgkueTXNPX/q4kz3TrvpAki//jSJLmcjoj/XuATbPabgb2VtUGYG/3nCSXAluAy7ptbk+yotvmDmAbsKH7N/s1JUlL7JShX1WPAi/Pat4M7OqWdwHX9bU/UFWvVNUh4CCwMckq4Lyq+kZVFfDFvm0kSWOy0NswXFRVxwCq6liSC7v21cBjff2Odm0/6pZntw+UZBu9TwWsW+cl829Ew26RIGlpLfaB3EHz9DVH+0BVtbOqpqtqempqatGKk6TWLTT0X+qmbOgej3ftR4G1ff3WAC927WsGtEuSxmihob8H2NotbwUe7GvfkuTsJJfQO2D7RDcV9IMkV3Zn7XywbxtJ0picck4/yf3AVcDKJEeBW4HbgN1JbgQOA9cDVNX+JLuB54ATwE1V9Wr3Uh+ldybQOcBXu3+SpDE6ZehX1fuHrLp6SP8dwI4B7fuAy+dVnSRpUXlFriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDzhpl4yS/BnwYKOAZ4EPAucB/ANYD3wX+UVX9j67/duBG4FXgY1X10Cj7lxbivscPD2y/4Yp1Y65EGr8Fj/STrAY+BkxX1eXACmALcDOwt6o2AHu75yS5tFt/GbAJuD3JitHKlyTNx6jTO2cB5yQ5i94I/0VgM7CrW78LuK5b3gw8UFWvVNUh4CCwccT9S5LmYcGhX1XfAz4HHAaOAX9RVQ8DF1XVsa7PMeDCbpPVwJG+lzjatZ0kybYk+5Lsm5mZWWiJkqRZRpneOZ/e6P0S4GLgTUk+MNcmA9pqUMeq2llV01U1PTU1tdASJUmzjDK98x7gUFXNVNWPgC8DPwe8lGQVQPd4vOt/FFjbt/0aetNBkqQxGSX0DwNXJjk3SYCrgQPAHmBr12cr8GC3vAfYkuTsJJcAG4AnRti/JGmeFnzKZlU9nuRLwLeAE8CTwE7gzcDuJDfSe2O4vuu/P8lu4Lmu/01V9eqI9UuS5mGk8/Sr6lbg1lnNr9Ab9Q/qvwPYMco+JUkL5xW5ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaMtLXJUqnct/jhyddgqQ+jvQlqSGGviQ1ZKTQT/KWJF9K8p0kB5L8bJILkjyS5IXu8fy+/tuTHEzyfJJrRi9fkjQfo470fxP4WlW9HfgZ4ABwM7C3qjYAe7vnJLkU2AJcBmwCbk+yYsT9S5LmYcGhn+Q84N3AXQBV9X+q6s+BzcCurtsu4LpueTPwQFW9UlWHgIPAxoXuX5I0f6OM9P8OMAP8VpInk9yZ5E3ARVV1DKB7vLDrvxo40rf90a7tJEm2JdmXZN/MzMwIJUqS+o0S+mcB7wTuqKp3AH9FN5UzRAa01aCOVbWzqqaranpqamqEEiVJ/UYJ/aPA0ap6vHv+JXpvAi8lWQXQPR7v67+2b/s1wIsj7F+SNE8LDv2q+j5wJMnbuqargeeAPcDWrm0r8GC3vAfYkuTsJJcAG4AnFrp/SdL8jXpF7j8D7k3yY8CfAh+i90ayO8mNwGHgeoCq2p9kN703hhPATVX16oj7lyTNw0ihX1VPAdMDVl09pP8OYMco+5QkLZz33pE6w+4TdMMV68ZcibR0vA2DJDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaMnLoJ1mR5MkkX+meX5DkkSQvdI/n9/XdnuRgkueTXDPqviVJ87MYI/2PAwf6nt8M7K2qDcDe7jlJLgW2AJcBm4Dbk6xYhP1Lkk7TSKGfZA3w94E7+5o3A7u65V3AdX3tD1TVK1V1CDgIbBxl/5Kk+Rl1pP9vgE8Br/W1XVRVxwC6xwu79tXAkb5+R7u2kyTZlmRfkn0zMzMjlihJ+msLDv0k7wOOV9WfnO4mA9pqUMeq2llV01U1PTU1tdASJUmznDXCtj8PXJvkvcCPA+cl+W3gpSSrqupYklXA8a7/UWBt3/ZrgBdH2L8kaZ4WHPpVtR3YDpDkKuCTVfWBJJ8FtgK3dY8PdpvsAe5L8nngYmAD8MSCK9eyct/jhyddgqTTMMpIf5jbgN1JbgQOA9cDVNX+JLuB54ATwE1V9eoS7F+SNMSihH5V/QHwB93ynwFXD+m3A9ixGPuUJM2fV+RKUkMMfUlqyFLM6UtvKMMOUt9wxboxVyKNzpG+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhriDdc0L35DlnRmc6QvSQ0x9CWpIYa+JDXE0JekhnggV1ogv1FLZ6IFj/STrE3y+0kOJNmf5ONd+wVJHknyQvd4ft8225McTPJ8kmsW4weQJJ2+UaZ3TgD/oqp+CrgSuCnJpcDNwN6q2gDs7Z7TrdsCXAZsAm5PsmKU4iVJ87Pg0K+qY1X1rW75B8ABYDWwGdjVddsFXNctbwYeqKpXquoQcBDYuND9S5Lmb1EO5CZZD7wDeBy4qKqOQe+NAbiw67YaONK32dGuTZI0JiOHfpI3A78DfKKq/nKurgPaashrbkuyL8m+mZmZUUuUJHVGCv0kf5Ne4N9bVV/uml9Ksqpbvwo43rUfBdb2bb4GeHHQ61bVzqqarqrpqampUUqUJPVZ8CmbSQLcBRyoqs/3rdoDbAVu6x4f7Gu/L8nngYuBDcATC92/tFx5KqeWs1HO0/954J8AzyR5qmv7l/TCfneSG4HDwPUAVbU/yW7gOXpn/txUVa+OsH9J0jwtOPSr6o8ZPE8PcPWQbXYAOxa6T0nSaLwNgyQ1xNCXpIZ47x1pTDzAq+XAkb4kNcTQl6SGGPqS1JAm5/SdW5XUKkf6ktQQQ1+SGmLoS1JDmpzTH8a5fk2Cv3caJ0P/NPhHqTcqf7fb4/SOJDXEkb60TA0bhYMjcS2coS+dgZyW0UI5vSNJDXGkLzVgrqmi+fT3k8SZz9DXQPMNCS0PS/3/zTeDM5+hPwL/ACSdaQx9SSNzAHTm8ECuJDXEkf4SOJNGPc7dS20Ze+gn2QT8JrACuLOqbht3DZPixTZSz3wHG/59LJ6xhn6SFcC/B34ROAp8M8meqnpunHUsR0v96cARvSbB37vlZ9wj/Y3Awar6U4AkDwCbgeZDfxj/aKQ3xpTpcql13KG/GjjS9/wocMXsTkm2Adu6p/8zyfNjqG2YlcB/n+D+58t6l5b1Lq151fvLS1jIaTrtesdc69Caxh36GdBWJzVU7QR2Ln05p5ZkX1VNT7qO02W9S8t6l5b1Lr1xn7J5FFjb93wN8OKYa5CkZo079L8JbEhySZIfA7YAe8ZcgyQ1a6zTO1V1IsmvAg/RO2Xz7qraP84aFmBZTDPNg/UuLetdWta7xFJ10pS6JOkNytswSFJDDH1JaoihP4ckv5Zkf5Jnk9yf5McnXdNckny8q3V/kk9Mup7Zktyd5HiSZ/vaLkjySJIXusfzJ1ljvyH1Xt/9930tybI6VW9IvZ9N8p0k307yu0neMsES/58htf7rrs6nkjyc5OJJ1thvUL196z6ZpJKsnERt82XoD5FkNfAxYLqqLqd34HnLZKsaLsnlwD+ld9XzzwDvS7JhslWd5B5g06y2m4G9VbUB2Ns9Xy7u4eR6nwV+CXh07NWc2j2cXO8jwOVV9dPAfwG2j7uoIe7h5Fo/W1U/XVV/F/gK8OvjLmoO93ByvSRZS++2MmfMpfOG/tzOAs5JchZwLsv7moKfAh6rqh9W1QngD4F/MOGaXqeqHgVentW8GdjVLe8CrhtnTXMZVG9VHaiqSV4hPtSQeh/ufh8AHqN3bczEDan1L/uevokBF25OypDfXYDfAD7FMqr1VAz9Iarqe8Dn6L2DHwP+oqoenmxVc3oWeHeSn0hyLvBeXn8h3HJ1UVUdA+geL5xwPW9kvwJ8ddJFzCXJjiRH6N21YDmN9E+S5Frge1X19KRrmQ9Df4hubnkzcAlwMfCmJB+YbFXDVdUB4DP0Ps5/DXgaODHnRmpGklvo/T7cO+la5lJVt1TVWnp1/uqk6xmmG1jdwjJ/YxrE0B/uPcChqpqpqh8BXwZ+bsI1zamq7qqqd1bVu+l9FH1h0jWdhpeSrALoHo9PuJ43nCRbgfcBv1xnzoU59wH/cNJFzOGt9AaETyf5Lr1ps28l+dsTreo0GPrDHQauTHJukgBXAwcmXNOcklzYPa6jd7Dx/slWdFr2AFu75a3AgxOs5Q2n+9KiTwPXVtUPJ13PXGadeHAt8J1J1XIqVfVMVV1YVeuraj29+4q9s6q+P+HSTskrcueQ5F8B/5jex+IngQ9X1SuTrWq4JH8E/ATwI+CfV9XeCZf0OknuB66idzval4Bbgf8I7AbW0Xujvb6qBh0wG7sh9b4M/FtgCvhz4KmqumZCJb7OkHq3A2cDf9Z1e6yqPjKRAvsMqfW9wNuA14D/BnykO7Y2cYPqraq7+tZ/l96Zfsv+NtaGviQ1xOkdSWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX1pESVZMugZpLoa+1Emyvrv3/J3d9xLcm+Q9Sb7e3e9/Y5K/193v/akkTyb5W0muSvL7Se4Dnul7nV3d/eG/1N2rRZo4L86SOknWAweBdwD7gW/Su3HdjfRuC/Ahet+rcFtVfT3Jm4H/DfwC8Hv07lt/qHudQ8AvdP3uBp6rqs+N+UeSTuJIX3q9Q919VV6jF/x7u5uUPQOsB74OfD7Jx4C39N2r/omqOtT3Okeq6uvd8m/Te2OQJs7Ql16v/95Kr/U9fw04q6puAz4MnAM8luTt3fq/mvU6sz9C+5Fay4KhL81Dkrd2nwQ+A+wD3j6k67okP9stvx/447EUKJ2CoS/Nzye6g7xPA/+L4d9EdQDYmuTbwAXAHeMqUJqLB3KlRdYdyP1KVV0+6Vqk2RzpS1JDHOlLUkMc6UtSQwx9SWqIoS9JDTH0Jakhhr4kNeT/AjFhyoect1ZEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(log_price, kde=False) \n",
    "# sns.displot(log_price, kde=False) \n",
    "# sns.histplot(log_price, kde=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-tanzania",
   "metadata": {},
   "source": [
    "As we see, this transformation removes the long tail, and now the distribution resembles a\n",
    "bell-shaped curve. This distribution is not normal, of course, because of the large peak in\n",
    "lower prices, but the model can deal with it more easily. \n",
    "\n",
    "**NOTE Generally, it’s good when the target distribution looks like the normal distribution (figure 2.8). Under\n",
    "this condition, models such as linear regression perform well.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "joint-saver",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-comedy",
   "metadata": {},
   "source": [
    "**Figure 2.8: The normal distribution, also known as Gaussian, follows the bell-shaped curve, which is symmetric\n",
    "and has a peak in the center.**\n",
    "\n",
    "#### Exercise 2.1\n",
    "\n",
    "\n",
    "The head of a distribution is a range where there are many values. What is a long tail of a\n",
    "distribution?\n",
    "\n",
    "\n",
    "a) A big peak around 1000 USD\n",
    "\n",
    "b) A case when many values are spread very far from the head — and these values\n",
    "visually appear as a “tail” on the histogram\n",
    "\n",
    "c) A lot of very similar values packed together within a short range\n",
    "\n",
    "### 2.2.4 Checking for missing values\n",
    "\n",
    "\n",
    "We will look more closely at other features a bit later, but one thing we should do now is check\n",
    "for missing values in the data. This step is important because typically, machine learning\n",
    "models cannot deal with missing values automatically. We need to know whether we need to\n",
    "do anything special to handle those values.\n",
    "\n",
    "\n",
    "Pandas has is a convenient function that checks for missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "confirmed-mexican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make                    0\n",
       "model                   0\n",
       "year                    0\n",
       "engine_fuel_type        3\n",
       "engine_hp              69\n",
       "engine_cylinders       30\n",
       "transmission_type       0\n",
       "driven_wheels           0\n",
       "number_of_doors         6\n",
       "market_category      3742\n",
       "vehicle_size            0\n",
       "vehicle_style           0\n",
       "highway_mpg             0\n",
       "city_mpg                0\n",
       "popularity              0\n",
       "msrp                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-thermal",
   "metadata": {},
   "source": [
    "The first thing we see is that MSRP — our target variable — doesn’t have any missing values.\n",
    "This result is good because otherwise, such records won’t be useful to us: we always need to\n",
    "know the target value of an observation to use it for training the model. Also, a few columns\n",
    "have missing values, especially market_category, in which we have almost 4,000 rows with\n",
    "missing values.\n",
    "\n",
    "\n",
    "We will need to deal with missing values later, when we train the model, so we should\n",
    "keep this problem in mind. For now, we won’t do anything else with these features and will\n",
    "proceed to the next step: setting up the validation framework so that we can train and test\n",
    "machine learning models.\n",
    "\n",
    "\n",
    "### 2.2.5 Validation framework\n",
    "\n",
    "\n",
    "As we learned previously, it’s important to set up the validation framework as early as possible\n",
    "to make sure that the models we train are good and can generalize. That is, that the model\n",
    "can be applied to new unseen data. To do that, we put aside some data and train the model\n",
    "only on one part. Then we use the held-out dataset — the one we didn’t use for training — to\n",
    "make sure that the predictions of the model make sense.\n",
    "\n",
    "\n",
    "It’s important because we train the model by using optimization methods that fit the\n",
    "function g(X) to the data X. Sometimes these optimization methods pick up spurious patterns\n",
    "— patterns that appear to be real patterns to the model but in reality are random fluctuations.\n",
    "If we have a small training dataset in which all BMW cars cost only $10,000, for example, the\n",
    "model will think that this is true for all BMW cars in the world.\n",
    "\n",
    "\n",
    "To ensure that this doesn’t happen, we use validation. Because the validation dataset is\n",
    "not used for training the model, the optimization method did not see this data. So when we\n",
    "apply the model to this data, it emulates the case of applying the model to new data that we\n",
    "never saw. If the validation dataset has BMW cars with prices higher than $10,000, but our model will predict 10,000 on them, we will notice that the model doesn’t perform well on\n",
    "these examples.\n",
    "\n",
    "\n",
    "As we already know, we need to split the dataset into three parts: train, validation, and\n",
    "test (figure 2.9)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "chemical-market",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-dream",
   "metadata": {},
   "source": [
    "**Figure 2.9 The entire dataset is split into three parts: train, validation and test.**\n",
    "\n",
    "Let’s split the dataframe such that:\n",
    "* 20% of data goes to validation,\n",
    "* 20% goes to test, and\n",
    "* the remaining 60% goes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "focal-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the dataframe. \n",
    "n = len(df) # A\n",
    "\n",
    "#Calculate how many rows should go to train, validation, and test. \n",
    "n_val = int(0.2 * n) # B\n",
    "n_test = int(0.2 * n) # B\n",
    "n_train = n - (n_val + n_test) # B\n",
    "\n",
    "#  Fix the random seed to make sure that the results are reproducible. \n",
    "np.random.seed(2) # C\n",
    "\n",
    "# Create a NumPy array with indices from 0 to (n-1) and shuffle it.\n",
    "idx = np.arange(n) # D\n",
    "np.random.shuffle(idx) # D\n",
    "\n",
    "#  Use the array with indices to get a shuffled dataframe. \n",
    "df_shuffled = df.iloc[idx] # E\n",
    "\n",
    "# Split the shuffled dataframe into train, validation, and test. \n",
    "df_train = df_shuffled.iloc[:n_train].copy() # F\n",
    "df_val = df_shuffled.iloc[n_train:n_train+n_val].copy() # F\n",
    "df_test = df_shuffled.iloc[n_train+n_val:].copy() # F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-creativity",
   "metadata": {},
   "source": [
    "Let’s take a closer look at this code and clarify a few things.\n",
    "\n",
    "\n",
    "In D, we create an array and then shuffle it. Let’s see what happens there. We can take a\n",
    "smaller array of five elements and shuffle it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "orange-denial",
   "metadata": {},
   "source": [
    "idx = np.arange(5)\n",
    "print('before shuffle', idx)\n",
    "np.random.shuffle(idx)\n",
    "print('after shuffle', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-display",
   "metadata": {},
   "source": [
    "If we run it, it prints something similar to "
   ]
  },
  {
   "cell_type": "raw",
   "id": "healthy-speech",
   "metadata": {},
   "source": [
    "before shuffle [0 1 2 3 4]\n",
    "after shuffle [2 3 0 4 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-contents",
   "metadata": {},
   "source": [
    "If we run it again, however, the results will be different:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tender-benjamin",
   "metadata": {},
   "source": [
    "before shuffle [0 1 2 3 4]\n",
    "after shuffle [4 3 0 2 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-amino",
   "metadata": {},
   "source": [
    "To make sure that every time we run it, the results are the same, in C we fix the random\n",
    "seed: \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "meaning-theology",
   "metadata": {},
   "source": [
    "np.random.seed(2)\n",
    "idx = np.arange(5)\n",
    "print('before shuffle', idx)\n",
    "np.random.shuffle(idx)\n",
    "print('after shuffle', idx) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-viewer",
   "metadata": {},
   "source": [
    "The function np.random.seed takes in any number and use this number as the starting seed\n",
    "for all the generated data inside NumPy’s random package.\n",
    "\n",
    "When we execute this code, it prints the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "distinct-discretion",
   "metadata": {},
   "source": [
    "before shuffle [0 1 2 3 4]\n",
    "after shuffle [2 4 1 3 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-compact",
   "metadata": {},
   "source": [
    "In this case the results are still random, but when we re-execute it, the result turns out to be\n",
    "the same as previously: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "official-placement",
   "metadata": {},
   "source": [
    "before shuffle [0 1 2 3 4]\n",
    "after shuffle [2 4 1 3 0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-constraint",
   "metadata": {},
   "source": [
    "This is good for reproducibility. If we want somebody else to run this code and get the same\n",
    "results, we need to make sure that everything is fixed, even the “random” component of our\n",
    "code.\n",
    "\n",
    "\n",
    "After we create an array with indices idx, we can use it to get a shuffled version of our\n",
    "initial dataframe. For that purpose in E, we use iloc, which is a way to access the rows of the\n",
    "dataframe by their numbers: \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "stretch-munich",
   "metadata": {},
   "source": [
    "df_shuffled = df.iloc[idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-poker",
   "metadata": {},
   "source": [
    "If idx contains shuffled consequent numbers, this code will produce a shuffled dataframe\n",
    "(figure 2.10)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "spectacular-mineral",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-turkey",
   "metadata": {},
   "source": [
    "**Figure 2.10 Using iloc to shuffle a dataframe. When used with a shuffled array of indices, it creates a shuffled\n",
    "dataframe.**\n",
    "\n",
    "In this example, we used iloc with a list of indices. We can also use ranges with the colon\n",
    "operator (:), and this is exactly what we do in F for splitting the shuffled dataframe into train,\n",
    "validation, and test: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "oriental-narrative",
   "metadata": {},
   "source": [
    "df_train = df_shuffled.iloc[:n_train].copy()\n",
    "df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()\n",
    "df_test = df_shuffled.iloc[n_train+n_val:].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-intersection",
   "metadata": {},
   "source": [
    "Now the dataframe is split into three parts, and we can continue. Our initial analysis showed a\n",
    "long tail in the distribution of prices, and to remove its effect, we need to apply the log\n",
    "transformation. We can do that for each dataframe separately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quiet-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(df_train.msrp.values)\n",
    "y_val = np.log1p(df_val.msrp.values)\n",
    "y_test = np.log1p(df_test.msrp.values) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-excess",
   "metadata": {},
   "source": [
    "To avoid accidentally using the target variable later, let’s remove it from the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "existing-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['msrp']\n",
    "del df_val['msrp']\n",
    "del df_test['msrp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-halifax",
   "metadata": {},
   "source": [
    "**NOTE: Removing the target variable is an optional step. But it’s helpful to make sure that we don’t use it\n",
    "when training a model: if it happens, we’d use price for predicting the price, and our model would have the\n",
    "perfect accuracy.**\n",
    "\n",
    "When the validation split is done, we can do the next step: training a model.\n",
    "    \n",
    "    \n",
    "## 2.3 Machine learning for regression\n",
    "\n",
    "\n",
    "After performing the initial data analysis, we are ready to train a model. The problem we are\n",
    "solving is a regression problem: the goal is to predict a number — the price of a car. For this\n",
    "project we will use the simplest regression model: linear regression.\n",
    "    \n",
    "    \n",
    "### 2.3.1 Linear regression\n",
    "\n",
    "\n",
    "To predict the price of a car we need to use a machine learning model. To do this, we will use\n",
    "linear regression, which we will implement ourselves. Typically, we don’t do this by hand;\n",
    "instead, we let a framework do this for us. In this chapter, however, we want to show that\n",
    "there is no magic inside these frameworks: it’s just code. Linear regression is a perfect model\n",
    "because it’s relatively simple and can be implemented with just a few lines of NumPy code.\n",
    "\n",
    "\n",
    "First, let’s understand how linear regression works. As we know from chapter 1, a\n",
    "supervised machine learning model has the form "
   ]
  },
  {
   "cell_type": "raw",
   "id": "familiar-cookbook",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-electricity",
   "metadata": {},
   "source": [
    "This is a matrix form. X is a matrix where the features of observations are rows of the matrix\n",
    "and y is a vector with the values we want to predict.\n",
    "\n",
    "\n",
    "These matrices and vectors may sound confusing, so let’s take a step back and consider\n",
    "what happens with a single observation xi and the value yi that we want to predict. The index i\n",
    "here means that this is an observation number i, one of m observations that we have in our\n",
    "training dataset.\n",
    "\n",
    "\n",
    "Then, for this single observation, the formula above looks like"
   ]
  },
  {
   "cell_type": "raw",
   "id": "liked-melbourne",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-destiny",
   "metadata": {},
   "source": [
    "If we have n features, our vector xi is n-dimensional, so it has n components:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "attended-stocks",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-pharmacology",
   "metadata": {},
   "source": [
    "Because it has n components, we can write the function g as a function with n parameters,\n",
    "which is the same as the previous formula: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "aboriginal-wrestling",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-failing",
   "metadata": {},
   "source": [
    "For our case, we have 7,150 cars in the training dataset. This means that m = 7150, and i can\n",
    "be any number between 0 and 7,149. For i = 10, for example, we have the following car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "limiting-astrology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make                                 rolls-royce\n",
       "model                     phantom_drophead_coupe\n",
       "year                                        2015\n",
       "engine_fuel_type     premium_unleaded_(required)\n",
       "engine_hp                                    453\n",
       "engine_cylinders                              12\n",
       "transmission_type                      automatic\n",
       "driven_wheels                   rear_wheel_drive\n",
       "number_of_doors                                2\n",
       "market_category        exotic,luxury,performance\n",
       "vehicle_size                               large\n",
       "vehicle_style                        convertible\n",
       "highway_mpg                                   19\n",
       "city_mpg                                      11\n",
       "popularity                                    86\n",
       "msrp                                      479775\n",
       "Name: 7557, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shuffled.iloc[10] # full record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chinese-weight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.08107460729463"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10] # logirithmic price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acoustic-tumor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479776.00000000006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(y_train[10]) # price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "golden-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make                                 rolls-royce\n",
       "model                     phantom_drophead_coupe\n",
       "year                                        2015\n",
       "engine_fuel_type     premium_unleaded_(required)\n",
       "engine_hp                                    453\n",
       "engine_cylinders                              12\n",
       "transmission_type                      automatic\n",
       "driven_wheels                   rear_wheel_drive\n",
       "number_of_doors                                2\n",
       "market_category        exotic,luxury,performance\n",
       "vehicle_size                               large\n",
       "vehicle_style                        convertible\n",
       "highway_mpg                                   19\n",
       "city_mpg                                      11\n",
       "popularity                                    86\n",
       "Name: 7557, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-sheriff",
   "metadata": {},
   "source": [
    "Let’s pick a few numerical features and ignore the rest for now. We can start with horsepower,\n",
    "mpg in the city, and popularity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "norman-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine_hp     453\n",
       "city_mpg       11\n",
       "popularity     86\n",
       "Name: 7557, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[10][['engine_hp','city_mpg','popularity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-celebration",
   "metadata": {},
   "source": [
    "Then let’s assign these features to xi1, xi2, and xi3, respectively. This way, we get the feature\n",
    "vector xi with 3 components: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "collect-prototype",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-wayne",
   "metadata": {},
   "source": [
    "To make it easier to understand, we can translate this mathematical notation to Python. In our\n",
    "case, the function g has the following signature:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sophisticated-accessory",
   "metadata": {},
   "source": [
    "def g(xi):\n",
    " # xi is a list with n elements\n",
    " # do something with xi\n",
    " # return the result\n",
    " pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-editor",
   "metadata": {},
   "source": [
    "In this code, the variable xi is our vector 𝑥𝑖. Depending on implementation, xi could be a list\n",
    "with n elements or a NumPy array of size n.\n",
    "\n",
    "\n",
    "For the car above, xi is a list with three elements: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "choice-serve",
   "metadata": {},
   "source": [
    "xi = [453, 11, 86]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-regulation",
   "metadata": {},
   "source": [
    "When we apply the function g to a vector xi, it produces y_pred as the output, which is the\n",
    "g’s prediction for xi: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "complicated-syndication",
   "metadata": {},
   "source": [
    "y_pred = g(xi) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-heritage",
   "metadata": {},
   "source": [
    "We expect this prediction to be as close as possible to yi, which is the real price of the car.\n",
    "\n",
    "\n",
    "**NOTE: In this section, we will use Python to illustrate the ideas behind mathematical formulas. We don’t\n",
    "need to use these code snippets for doing the project. On the other hand, taking this code, putting it into\n",
    "Jupyter, and trying to run it could be helpful for understanding the concepts.**\n",
    "\n",
    "\n",
    "There are many ways the function g could look, and the choice of a machine learning\n",
    "algorithm defines the way it works.\n",
    "\n",
    "\n",
    "If g is the linear regression model, it has the following form: \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "personalized-serial",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-ceremony",
   "metadata": {},
   "source": [
    "The variables w0, w1, w2, …, wn are the parameters of the model:\n",
    "\n",
    "* w0 is the bias term.\n",
    "* w1, w2, …, wn are the weights for each feature xi1, xi2, …, xin.\n",
    "\n",
    "These parameters define exactly how the model should combine the features so that the\n",
    "predictions at the end are as good as possible. It’s okay if the meaning behind these\n",
    "parameters is not clear yet, since we will cover them later in this section.\n",
    "\n",
    "To keep the formula shorter, let’s use sum notation: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "exposed-discharge",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-virtue",
   "metadata": {},
   "source": [
    "#### Exercise 2.2\n",
    "\n",
    "For supervised learning, a machine learning model for a single observation yi ≈ g(xi). What\n",
    "are xi and yi for this project?\n",
    "\n",
    "**a) xi is a feature vector - a vector that contains a few numbers that describe the\n",
    "object (a car), and yi is the logarithm of the price of this car**\n",
    "\n",
    "b) yi is a feature vector - a vector that contains a few numbers that describe the\n",
    "object (a car), and xi is the logarithm of the price of this car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "convinced-interval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[453.0, 11, 86]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi1 = df_train.iloc[10][['engine_hp','city_mpg','popularity']][0]\n",
    "xi2 = df_train.iloc[10][['engine_hp','city_mpg','popularity']][1]\n",
    "xi3 = df_train.iloc[10][['engine_hp','city_mpg','popularity']][2]\n",
    "xi = [xi1,xi2,xi3]\n",
    "xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "circular-health",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.08107460729463"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yi = y_train[10]\n",
    "yi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-banks",
   "metadata": {},
   "source": [
    "These weights are what the model learns when we train it. We can have a model with the\n",
    "following weights, for example (table 2.1).\n",
    "\n",
    "\n",
    "**Table 2.1 An example of weights that a linear regression model learned**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "affected-tuesday",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-reply",
   "metadata": {},
   "source": [
    "So if we want to translate this model to Python, this is how it will look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "infrared-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 7.17\n",
    "w = [0.01, 0.04, 0.002] # [w1 w2 w3 ]\n",
    "n = 3\n",
    "def linear_regression(xi):\n",
    "    result = w0\n",
    "    for j in range(n):\n",
    "        result = result + xi[j] * w[j]\n",
    "#         print(xi[j])\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-solid",
   "metadata": {},
   "source": [
    "We put all the feature weights inside a single list w — just like we did with xi previously. All\n",
    "we need to do now is loop over these weights and multiply them by the corresponding feature\n",
    "values. This is nothing else, but the direct translation of the formula above to Python.\n",
    "This is easy to see. Have another look at the formula: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "monthly-arrow",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-tooth",
   "metadata": {},
   "source": [
    "Our example has three features, so n = 3, and we have"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mature-pixel",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-haven",
   "metadata": {},
   "source": [
    "This is exactly what we have in the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "focused-flush",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.312"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual\n",
    "result = w0 + xi[0] * w[0] + xi[1] * w[1] + xi[2] * w[2]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-verse",
   "metadata": {},
   "source": [
    "With the simple exception that indexing in Python starts with 0, xi1 becomes xi[0] and w1 is\n",
    "w[0].\n",
    "\n",
    "Now let’s see what happens when we apply the model to our observation xi and replace the\n",
    "weights with their values: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "champion-sheep",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-opera",
   "metadata": {},
   "source": [
    "The prediction we get for this observation is 12.31. Remember that during preprocessing, we\n",
    "applied the logarithmic transformation to our target variable y. This is why the model we\n",
    "trained on this data also predicts the logarithm of the price. To undo the transformation, we\n",
    "need to take the exponent of the logarithm. In our case, when we do it, the prediction\n",
    "becomes 220,000.\n",
    "\n",
    "\n",
    "The bias term (7.17) is the value we would predict if we didn’t know anything about the\n",
    "car; it serves as a baseline.\n",
    "\n",
    "\n",
    "We do know something about the car, however: horsepower, mpg in the city, and\n",
    "popularity. These features are the xi1, xi2, and xi3 features, each of which tells us something\n",
    "about the car. We use this information to adjust the baseline.\n",
    "\n",
    "\n",
    "Let’s consider the first feature: horsepower. The weight for this feature is 0.01, which\n",
    "means that for each extra unit of horsepower, we adjust the baseline by adding 0.01. Because\n",
    "we have 453 horses in the engine, we add 4.53 to the baseline: 453 horses ⋅ 0.01 = 4.53.\n",
    "The same happens with mpg. Each additional mpg increases the price by 0.04, so we add\n",
    "0.44: 11 mpg ⋅ 0.04 = 0.44.\n",
    "\n",
    "\n",
    "Finally, we take popularity into account. In our example, each mention in the Twitter\n",
    "stream results in a 0.002 increase. In total, popularity contributes 0.172 to the final\n",
    "prediction.\n",
    "\n",
    "\n",
    "This is exactly why we get 12.31 when we combine everything (figure 2.11). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ancient-royalty",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-savannah",
   "metadata": {},
   "source": [
    "**Figure 2.11 The prediction of linear regression is the baseline of 7.17 (the bias term) adjusted by information we\n",
    "have from the features. Horsepower contributes 4.53 to the final prediction; mpg, 0.44; and popularity, 0.172.**\n",
    "\n",
    "\n",
    "Now let’s remember that we are actually dealing with vectors, not individual numbers. We\n",
    "know that xi is a vector with n components: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "practical-rochester",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-martial",
   "metadata": {},
   "source": [
    "We can also put all the weights together in a single vector w: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "perfect-sending",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-alert",
   "metadata": {},
   "source": [
    "In fact, we already did that in the Python example when we put all the weights in a list, which\n",
    "was a vector of dimensionality 3 with weights for each individual feature. This is how the\n",
    "vectors look like for our example: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "creative-hearts",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-marshall",
   "metadata": {},
   "source": [
    "Because we now think of both features and weights as vectors xi and w, respectively, we can\n",
    "replace the sum of the elements of these vectors with a dot product between them: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dying-bottle",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-sword",
   "metadata": {},
   "source": [
    "The dot product is a way of multiplying two vectors: we multiply corresponding elements of\n",
    "the vectors and then sum the results. Refer to Appendix C for more details about vectorvector multiplication.\n",
    "\n",
    "\n",
    "The translation of the formula for dot product to the code is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "continent-vacation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53\n",
      "4.970000000000001\n",
      "5.142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.142"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot(xi, w):\n",
    "    n = len(w)\n",
    "    result = 0.0\n",
    "    for j in range(n):\n",
    "        result = result + xi[j] * w[j]\n",
    "        print(result)\n",
    "    return result\n",
    "dot(xi, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-target",
   "metadata": {},
   "source": [
    "Using the new notation, we can rewrite the entire equation for linear regression as "
   ]
  },
  {
   "cell_type": "raw",
   "id": "adolescent-february",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-radio",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* w0 is the bias term.\n",
    "* w is the n-dimensional vector of weights.\n",
    "\n",
    "Now we can use the new dot function, so the linear regression function in Python becomes\n",
    "very short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fixed-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53\n",
      "4.970000000000001\n",
      "5.142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.312000000000001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_regression(xi):\n",
    "     return w0 + dot(xi, w) \n",
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-custom",
   "metadata": {},
   "source": [
    "Alternatively, if xi and w are NumPy arrays, we can use the built-in dot method for multiplication:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "raising-farming",
   "metadata": {},
   "source": [
    "def linear_regression(xi):\n",
    "     return w0 + xi.dot(w)\n",
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-hierarchy",
   "metadata": {},
   "source": [
    "To make it even shorter, we can combine w0 and w into one (n+1)-dimensional vector by\n",
    "prepending w0 to w right in front of w1: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "structural-metadata",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-cassette",
   "metadata": {},
   "source": [
    "Here, we have a new weights vector w that consists of the bias term w0 followed by the\n",
    "weights w1, w2, ... from the original weights vector w.\n",
    "\n",
    "\n",
    "In Python, this is very easy to do. If we already have the old weights in a list w, all we\n",
    "need to do is the following:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "regional-description",
   "metadata": {},
   "source": [
    "w = [w0] + w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-counter",
   "metadata": {},
   "source": [
    "Remember that the plus operator in Python concatenates lists, so [1] + [2, 3, 4] will create\n",
    "a new list with 4 elements: [1, 2, 3, 4]. In our case, w is already a list, so we create a new\n",
    "w with one extra element at the beginning: w0.\n",
    "\n",
    "\n",
    "Because now w becomes a (n+1)-dimensional vector, we also need to adjust the feature\n",
    "vector xi so that the dot product between them still works. We can do this easily by adding a\n",
    "dummy feature xi0, which always takes the value 1. Then we prepend this new dummy feature\n",
    "to xi right before xi1: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "broke-extra",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-provincial",
   "metadata": {},
   "source": [
    "Or, in code:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "related-parade",
   "metadata": {},
   "source": [
    "xi = [1] + xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-sharing",
   "metadata": {},
   "source": [
    "So we create a new list xi with 1 as the first element followed by all the elements from the old\n",
    "list xi.\n",
    "\n",
    "\n",
    "With these modifications, we can express the model as the dot product between the new xi\n",
    "and the new w: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "mighty-tampa",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-ministry",
   "metadata": {},
   "source": [
    "The translation to the code is simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "educated-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.17\n",
      "11.7\n",
      "12.139999999999999\n",
      "12.312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.312"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 7.17\n",
    "w = [0.01, 0.04, 0.002]\n",
    "w = [w0] + w\n",
    "\n",
    "def linear_regression(xi):\n",
    "    xi = [1] + xi\n",
    "    return dot(xi, w) \n",
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-gateway",
   "metadata": {},
   "source": [
    "These formulas for linear regressions are equivalent because the first feature of the new xi is\n",
    "1, so when we multiply the first component of xi by the first component of w, we get the bias\n",
    "term, because w0 ⋅ 1 = w0.\n",
    "\n",
    "\n",
    "We are ready to consider the bigger picture again and talk about the matrix form. There\n",
    "are many observations and xi is one of them. Thus, we have m feature vectors x1, x2, …, xi,\n",
    "…, xm, and each of these vectors consists of n+1 features: \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ordinary-catalog",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-insulation",
   "metadata": {},
   "source": [
    "We can put these vectors together as rows of a matrix. Let’s call this matrix X (figure 2.12). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "wired-sheep",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-court",
   "metadata": {},
   "source": [
    "**Figure 2.12 Matrix X, in which observations x1, x2, …, xm are rows**\n",
    "\n",
    "Let’s see how it looks in code. We can take a few rows from the training dataset, such as the\n",
    "first, second, and tenth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acknowledged-space",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.57574708,  9.887663  , 13.08107461])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = y_train[0]\n",
    "y2 = y_train[1]\n",
    "y10 = y_train[10]\n",
    "y = np.array([y1,y2,y10])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nominated-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1, 148, 24, 1385]\n",
    "x2 = [1, 132, 25, 2031]\n",
    "x10 = [1, 453, 11, 86] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-aggregate",
   "metadata": {},
   "source": [
    "Now let’s put the rows together in another list: \n",
    "\n",
    "List X now contains three lists. We can think of it as a 3x4 matrix — a matrix with three rows\n",
    "and four columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "asian-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 148, 24, 1385], [1, 132, 25, 2031], [1, 453, 11, 86]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [x1, x2, x10]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "alternative-department",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  148,   24, 1385],\n",
       "       [   1,  132,   25, 2031],\n",
       "       [   1,  453,   11,   86]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([x1, x2, x10])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-version",
   "metadata": {},
   "source": [
    "Each column of this matrix is a feature:\n",
    "    \n",
    "1. The first column is a dummy feature with “1”\n",
    "\n",
    "2. The second column is the engine horsepower\n",
    "\n",
    "3. The third — MPG in the city\n",
    "\n",
    "4. And the last one — popularity, or the number of mentions in a Twitter stream\n",
    "\n",
    "We already learned that to make a prediction for a single feature vector, we need to calculate\n",
    "the dot product between this feature vector and the weights vector. Now we have a matrix X,\n",
    "which in Python is a list of feature vectors. To make predictions for all the rows of the matrix,\n",
    "we can simply iterate over all rows of X and compute the dot product: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "covered-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.17\n",
      "8.65\n",
      "9.61\n",
      "12.379999999999999\n",
      "7.17\n",
      "8.49\n",
      "9.49\n",
      "13.552\n",
      "7.17\n",
      "11.7\n",
      "12.139999999999999\n",
      "12.312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.379999999999999, 13.552, 12.312]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([237993.82334859, 768349.51018973, 222348.22211011])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = []\n",
    "for xi in X:\n",
    "    pred = dot(xi, w)\n",
    "    predictions.append(pred) \n",
    "    \n",
    "display(predictions)\n",
    "display(np.exp(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-scanner",
   "metadata": {},
   "source": [
    "In linear algebra, this is the matrix-vector multiplication: we multiply the matrix X by the\n",
    "vector w. The formula for linear regression becomes "
   ]
  },
  {
   "cell_type": "raw",
   "id": "alpha-turkish",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-provision",
   "metadata": {},
   "source": [
    "The result is an array with predictions for each row of X. Refer to appendix C for more details\n",
    "about matrix-vector multiplication.\n",
    "With this matrix formulation, the code for applying linear regression to make predictions\n",
    "becomes very simple. The translation to NumPy becomes straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "minute-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.38 , 13.552, 12.312])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([237993.82334859, 768349.51018973, 222348.22211011])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predictions = np.dot(X,w)\n",
    "\n",
    "predictions = X.dot(w) \n",
    "\n",
    "\n",
    "display(predictions)\n",
    "display(np.exp(predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-skill",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "\n",
    "When we multiply the matrix X by the weights vector w, we get\n",
    "\n",
    "a) A vector y with the actual price\n",
    "\n",
    "b) A vector y with price predictions\n",
    "\n",
    "c) A single number y with price predictions\n",
    "\n",
    "### 2.3.2 Training linear regression model\n",
    "\n",
    "So far, we’ve only covered making predictions. To be able to do that, we need to know the\n",
    "weights w. How do we get them?\n",
    "\n",
    "We learn the weights from data: we use the target variable y to find such w that combines\n",
    "the features of X in the best possible way. “Best possible” in the case of linear regression\n",
    "means that it minimizes the error between the predictions g(X) and the actual target y.\n",
    "\n",
    "There are multiple ways to do that. We will use normal equation, which is the simplest\n",
    "method to implement. The weight vector 𝑤 can be computed with the following formula:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "injured-yahoo",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-momentum",
   "metadata": {},
   "source": [
    "**NOTE: Covering the derivation of the normal equation is out of scope for this book. We give a bit of intuition\n",
    "of how it works in appendix C, but you should consult a machine learning textbook for a more in-depth\n",
    "introduction.**\n",
    "\n",
    "This piece of math may appear scary or confusing, but it’s quite easy to translate to NumPy:  \n",
    "    \n",
    "* T is the transpose of X. In NumPy, it’s X.T\n",
    "* TX is a matrix–matrix multiplication, which we can do with the dot method from\n",
    "NumPy: X.T.dot(X).\n",
    "* -1 is the inverse of X. We can use np.linalg.inv function to calculate the inverse.\n",
    "So the formula above translates directly to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "micro-following",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11696847, 0.02969558, 0.30901491, 0.00066636])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inv(X.T.dot(X)).dot(X.T).dot(y) # page 48\n",
    "np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-jacksonville",
   "metadata": {},
   "source": [
    "Please refer to appendix C for more details about this equation.\n",
    "\n",
    "To implement the normal equation, we need to do the following: \n",
    "\n",
    "1. Create a function that takes in a matrix X with features and a vector y with the target.\n",
    "2. Add a dummy column (the feature that is always set to 1) to the matrix X.\n",
    "3. Train the model: compute the weights w by using the normal equation.\n",
    "4. Split this w into the bias w0 and the rest of the weights, and return them.\n",
    "\n",
    "The last step — splitting w into the bias term and the rest — is optional and mostly for\n",
    "convenience; otherwise, we need to add the dummy column every time we want to make\n",
    "predictions instead of doing it once during training.\n",
    "\n",
    "Let’s implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "specified-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 148,   24, 1385],\n",
       "       [ 132,   25, 2031],\n",
       "       [ 453,   11,   86]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = [148, 24, 1385]\n",
    "x2 = [132, 25, 2031]\n",
    "x10 = [453, 11, 86] \n",
    "X = np.array([x1, x2, x10])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wrong-picking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.57574708,  9.887663  , 13.08107461])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "alien-reverse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11696846975654829, array([0.02969558, 0.30901491, 0.00066636]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_regression(X, y):\n",
    "    # adding the dummy column\n",
    "    ones = np.ones(X.shape[0]) # A\n",
    "    X = np.column_stack([ones, X]) # B\n",
    "\n",
    "    # normal equation formula\n",
    "    XTX = X.T.dot(X) # C\n",
    "    XTX_inv = np.linalg.inv(XTX) # D\n",
    "    w = XTX_inv.dot(X.T).dot(y) # E\n",
    "\n",
    "    return w[0], w[1:] # F\n",
    "linear_regression(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-fabric",
   "metadata": {},
   "source": [
    "With six lines of code, we have implemented our first machine learning algorithm. In A, we\n",
    "create a vector containing only ones, which we append to the matrix X as the first column;\n",
    "this is the dummy feature in B. Next, we compute XTX in C and its inverse in D, and we put\n",
    "them together to calculate w in E. Finally, we split the weights into the bias w0 and the\n",
    "remaining weights w in F.\n",
    "\n",
    "\n",
    "The column_stack function from NumPy that we used for adding a column of ones might\n",
    "be confusing at first, so let’s have a closer look at it:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "retained-major",
   "metadata": {},
   "source": [
    "np.column_stack([ones, X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-harvard",
   "metadata": {},
   "source": [
    "It takes in a list of NumPy arrays, which in our case contains ones and X and stacks them\n",
    "(figure 2.13). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "earned-overhead",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-moses",
   "metadata": {},
   "source": [
    "**Figure 2.13 The function column_stack takes a list of NumPy arrays and stacks them in columns. In our case,\n",
    "the function appends the array with ones as the first column of the matrix.**\n",
    "\n",
    "If weights are split into the bias term and the rest, the linear regression formula for making\n",
    "predictions changes slightly: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "german-motion",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-delicious",
   "metadata": {},
   "source": [
    "This is still very easy to translate to NumPy: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "wanted-kingdom",
   "metadata": {},
   "source": [
    "y_pred = w0 + X.dot(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-toyota",
   "metadata": {},
   "source": [
    "Let’s use it for our project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-creature",
   "metadata": {},
   "source": [
    "## 2.4 Predicting the price\n",
    "\n",
    "We’ve covered a great deal of theory, so let’s come back to our project: predicting the price of\n",
    "a car. We now have a function for training a linear regression model at our disposal, so let’s\n",
    "use it to build a simple baseline solution. \n",
    "\n",
    "\n",
    "### 2.4.1 Baseline solution\n",
    "\n",
    "To be able to use it, however, we need to have some data: a matrix X and a vector with the\n",
    "target variable y. We have already prepared the y, but we still don’t have the X: what we haveright now is a data frame, not a matrix. So we need to extract some features from our dataset\n",
    "to create this matrix X.\n",
    "\n",
    "\n",
    "We will start with a very naive way of creating features: select a few numerical features\n",
    "and form the matrix X from them. In the example previously, we used only three features.\n",
    "This time, we include a couple more features and use the following columns:\n",
    "\n",
    "* engine_hp\n",
    "* engine_cylinders\n",
    "* highway_mpg\n",
    "* city_mpg\n",
    "* popularity \n",
    "\n",
    "\n",
    "Let’s select the features from the data frame and write them to a new variable, df_num:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlike-router",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine_hp           40\n",
       "engine_cylinders    14\n",
       "highway_mpg          0\n",
       "city_mpg             0\n",
       "popularity           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  declare a variable named, 'base'. set the value to the columns above.\n",
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg','popularity']\n",
    "\n",
    "#  declare a variable named, 'df_num'. set the value to the 'base' columns of the df_train.\n",
    "df_num = df_train[base] \n",
    "\n",
    "#  test for Nan\n",
    "# df_num.iloc[350:400]\n",
    "\n",
    "# count the nulls of the df_num dataframe.\n",
    "df_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-touch",
   "metadata": {},
   "source": [
    "As discussed in the section on exploratory data analysis, the dataset has missing values. We\n",
    "need to do something because the linear regression model cannot deal with missing values\n",
    "automatically.\n",
    "\n",
    "\n",
    "One option is to drop all the rows that contain at least one missing value. This approach,\n",
    "however, has some disadvantages. Most important, we will lose the information that we have\n",
    "in the other columns. Even though we may not know the number of doors of a car, we still\n",
    "know other things about the car, such as make, model, age, and other things that we don’t\n",
    "want to throw away.\n",
    "\n",
    "\n",
    "The other option is filling the missing values with some other value. This way, we don’t\n",
    "lose the information in other columns and still can make predictions even if the row has\n",
    "missing values. The simplest possible approach is to fill the missing values with zero. We can\n",
    "use the fillna method from Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "tutorial-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the nulls of the df_num dataframe to zero.\n",
    "df_num = df_num.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-scout",
   "metadata": {},
   "source": [
    "This method may not be the best way to deal with missing values, but often, it’s good enough.\n",
    "If we set the missing feature value to zero, the respective feature is simply ignored. \n",
    "\n",
    "**NOTE: An alternative option is to replace the missing values with the average values. For some variables, for\n",
    "example, the number of cylinders, the value of zero doesn’t make much sense: a car cannot have zero\n",
    "cylinders. However, this will make our code more complex, and won’t have a significant impact on the results.**\n",
    "\n",
    "That’s why we follow a simpler approach and replace the missing values with zeros.\n",
    "It’s not difficult to see why setting a feature to zero is the same as ignoring it. Let’s recall the\n",
    "formula for linear regression. In our case, we have five features, so the formula is "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dense-sustainability",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-limit",
   "metadata": {},
   "source": [
    "If feature 3 is missing, and we fill it with zero, 𝑥𝑖3 becomes zero:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nominated-campbell",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-olive",
   "metadata": {},
   "source": [
    "In this case, regardless of the weight w3 for this feature, the product xi3w3 will always be zero.\n",
    "In other words, this feature will have no contribution to the final prediction, and we will base\n",
    "our prediction only on features that aren’t missing: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dramatic-tomato",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-jackson",
   "metadata": {},
   "source": [
    "Now we need to convert this dataframe to a NumPy array. The easiest way to do it is to use its\n",
    "values property: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "endangered-vinyl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 148.,    4.,   33.,   24., 1385.],\n",
       "       [ 132.,    4.,   32.,   25., 2031.],\n",
       "       [ 148.,    4.,   37.,   28.,  640.],\n",
       "       ...,\n",
       "       [ 285.,    6.,   22.,   17.,  549.],\n",
       "       [ 563.,   12.,   21.,   13.,   86.],\n",
       "       [ 200.,    4.,   31.,   22.,  873.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  declare a variable name, X_train. set the value to the values of df_num\n",
    "X_train = df_num.values\n",
    "\n",
    "# call the X_train variable \n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-paris",
   "metadata": {},
   "source": [
    "X_train is a matrix — a two-dimensional NumPy array. It’s something we can use as input to\n",
    "our linear_regresson function. Let’s call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "intelligent-bunny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.927257388070117,\n",
       " array([ 9.70589522e-03, -1.59103494e-01,  1.43792133e-02,  1.49441072e-02,\n",
       "        -9.06908672e-06]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "w_0, w = linear_regression(X_train, y_train) \n",
    "w_0, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-rover",
   "metadata": {},
   "source": [
    "We have just trained the first model! Now we can apply it to the training data to see how well\n",
    "it predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "affiliated-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.54792783,  9.38733977,  9.67197758, ..., 10.30423015,\n",
       "       11.9778914 ,  9.99863111])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = w_0 + X_train.dot(w) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-arizona",
   "metadata": {},
   "source": [
    "To see how good the predictions are, we can use distplot — a function form Seaborn for\n",
    "plotting histograms that we used previously — to plot the predicted values and compare them\n",
    "with the actual prices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "surface-latitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bizof\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2767f812b08>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y_pred, label='prediction')\n",
    "sns.distplot(y_train, label='target')\n",
    "plt.legend() \n",
    "\n",
    "# sns.displot(y_pred, label='prediction')\n",
    "# sns.displot(y_train, label='target')\n",
    "# plt.legend() \n",
    "\n",
    "# sns.histplot(y_pred, label='prediction')\n",
    "# sns.histplot(y_train, label='target')\n",
    "# plt.legend() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-inspector",
   "metadata": {},
   "source": [
    "We can see from the plot (figure 2.14) that the distribution of values we predicted looks quite\n",
    "different from the actual values. This result may indicate that the model is not powerful\n",
    "enough to capture the distribution of the target variable. This shouldn’t be a surprise to us:\n",
    "the model we used is quite basic and includes only five very simple features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-machinery",
   "metadata": {},
   "source": [
    "**Figure 2.14 The distribution of the predicted values (light gray) and the actual values (dark gray). We see that\n",
    "our predictions aren’t quite good; they are very different from the actual distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-addition",
   "metadata": {},
   "source": [
    "### 2.4.2 RMSE: evaluating model quality\n",
    "\n",
    "\n",
    "Looking at plots and comparing the distributions of the actual target variable with the\n",
    "predictions is a good way to evaluate quality, but we cannot do this every time we change\n",
    "something in the model. Instead, we need to use a metric that quantifies the quality of the\n",
    "model. We can use many metrics to evaluate how well a regression model behaves. The most\n",
    "commonly used one is root mean squared error — RMSE for short.\n",
    "\n",
    "\n",
    "RMSE tells us how large are the errors that our model makes. It’s computed with the\n",
    "following formula:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "refined-flush",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-permission",
   "metadata": {},
   "source": [
    "Let’s try to understand what’s going on here. First, let’s look inside the sum. We have \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "graduate-montana",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-authentication",
   "metadata": {},
   "source": [
    "This is the difference between 𝑔(𝑥𝑖) — the prediction we make for the observation 𝑥𝑖 — and the\n",
    "actual target value 𝑦𝑖 for that observation (figure 2.15). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "annual-composition",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-cable",
   "metadata": {},
   "source": [
    "**Figure 2.15 The difference between the predictions g(xi) and the actual values yi.**\n",
    "\n",
    "Then we use the square of the difference, which gives a lot more weight to larger differences.\n",
    "If we predict 9.5, for example, and the actual value is 9.6, the difference is 0.1, so its square\n",
    "is 0.01, which is quite small. But if we predict 7.3, and the actual value is 10.3, the difference\n",
    "is 3, and the square of the difference is 9 (figure 2.16). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "annoying-charge",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-melissa",
   "metadata": {},
   "source": [
    "**Figure 2.16 The square of the difference between the predictions and the actual values. For large differences,\n",
    "the square is quite big.**\n",
    "\n",
    "This is the SE part (squared error) of RMSE.\n",
    "Next, we have a sum: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "classified-judges",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-assessment",
   "metadata": {},
   "source": [
    "This summation goes over all m observations and puts all the squared errors together (figure\n",
    "2.17) into a single number. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "placed-cabinet",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-roots",
   "metadata": {},
   "source": [
    "**Figure 2.17 The result of the summation of all the square differences is a single number.**\n",
    "\n",
    "If we divide this sum by 𝑚, we get the mean squared error: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "mathematical-celtic",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-object",
   "metadata": {},
   "source": [
    "This is the squared error that our model makes on average — the M part (mean) of RMSE, or\n",
    "mean squared error (MSE). MSE is also a good metric on its own (figure 2.18). \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dental-fever",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-paragraph",
   "metadata": {},
   "source": [
    "Figure 2.18 MSE is computed by calculating the mean of the squared errors.\n",
    "\n",
    "Finally, we take the square root of that: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "disciplinary-niagara",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-emission",
   "metadata": {},
   "source": [
    "This is the R part (root) of RMSE (figure 2.19). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "italic-lottery",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-sarah",
   "metadata": {},
   "source": [
    "**Figure 2.19 RMSE: we first compute MSE and then calculate its square root.**\n",
    "\n",
    "When using NumPy to implement RMSE, we can take advantage of vectorization: the process\n",
    "of applying the same operation to all elements of one or more NumPy arrays. We get multiple\n",
    "benefits from using vectorization. First, the code is more concise: we don’t have to write any\n",
    "loops to apply the same operation to each element of the array. Second, vectorized operations\n",
    "are a lot faster than simple Python for loops.\n",
    "Consider the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "brief-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    error = y_pred - y # A\n",
    "    mse = (error ** 2).mean() # B\n",
    "    return np.sqrt(mse) # C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-texas",
   "metadata": {},
   "source": [
    "In A, we compute elementwise difference between the vector with predictions and the vector\n",
    "with the target variable. The result is a new NumPy array error that contains the differences.\n",
    "In B, we do two operations in one line: compute the square of each element of the error\n",
    "array and then get the mean value of the result, which gives us MSE. In C, we compute the\n",
    "square root to get RMSE.\n",
    "\n",
    "\n",
    "Elementwise operations in NumPy and Pandas are quite convenient. We can apply an\n",
    "operation to an entire NumPy array (or a Pandas series) without writing loops.\n",
    "\n",
    "\n",
    "In the first line of our rmse function, for example, we compute the difference between the\n",
    "predictions and the actual prices: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "secret-island",
   "metadata": {},
   "source": [
    "error = y_pred - y\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-ebony",
   "metadata": {},
   "source": [
    "What happens here is that for each element of y_pred, we subtract the corresponding element\n",
    "of y and then put the result to the new array error (figure 2.20). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "clear-miller",
   "metadata": {},
   "source": [
    "place image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-porter",
   "metadata": {},
   "source": [
    "**Figure 2.20 The elementwise difference between y_pred and y. The result is written to the error array.**\n",
    "\n",
    "Next, we compute the square of each element of the error array and then calculate its mean\n",
    "to get the mean squared error of our model (figure 2.21)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "macro-orchestra",
   "metadata": {},
   "source": [
    "place formula here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-brunei",
   "metadata": {},
   "source": [
    "**Figure 2.21 To calculate MSE, we first compute the square of each element in the error array and then compute\n",
    "the mean value of the result.**\n",
    "\n",
    "To see what exactly happens, we need to know that the power operator (**) is also applied\n",
    "elementwise, so the result is another array in which all elements of the original array are\n",
    "squared. When we have this new array with squared elements, we simply compute its mean\n",
    "by using the mean() method (figure 2.22). "
   ]
  },
  {
   "cell_type": "raw",
   "id": "drawn-james",
   "metadata": {},
   "source": [
    "place formula / image here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-broadcast",
   "metadata": {},
   "source": [
    "**Figure 2.22 The power operator (\\**) applied elementwise to the error array. The result is another array in which\n",
    "each element is squared. Then we compute the mean of the array with the squared error to compute MSE.**]\n",
    "\n",
    "Finally, we compute the square root of the mean value to get RMSE: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "burning-tennis",
   "metadata": {},
   "source": [
    "np.sqrt(mse) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-project",
   "metadata": {},
   "source": [
    "Now we can use RMSE to evaluate the quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "excellent-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7554192603920132"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_train, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-expression",
   "metadata": {},
   "source": [
    "The code prints 0.75.\n",
    "\n",
    "This number tells us that on average, the model’s predictions are off by 0.75. This result\n",
    "alone may not be very useful, but we can use it to compare this model with other models. If\n",
    "one model has a better (lower) RMSE than the other, it indicates that this model is better. \n",
    "\n",
    "## 2.4.3 Validating the model\n",
    "\n",
    "\n",
    "In the example from the previous section we computed RMSE on the training set. The result is\n",
    "useful to know but doesn’t reflect the way the model will be used later. The model will be used\n",
    "to predict the price of cars that it didn’t see before. For that purpose, we set aside a validation\n",
    "dataset. We intentionally don’t use it for training and keep it for validating the model.\n",
    "\n",
    "\n",
    "We have already split our data into multiple parts: df_train, df_val, and df_test. We\n",
    "have also created a matrix X_train from df_train and used X_train and y_train to train\n",
    "the model. Now we need to do the same steps to get X_val — a matrix with features\n",
    "computed from the validation dataset. Then we can apply the model to X_val to get\n",
    "predictions and compare them with y_val.\n",
    "\n",
    "\n",
    "First, we create the X_val matrix, following the same steps as for X_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "other-salem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine_hp           16\n",
       "engine_cylinders     7\n",
       "highway_mpg          0\n",
       "city_mpg             0\n",
       "popularity           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num = df_val[base] \n",
    "df_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "better-assets",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine_hp           0\n",
       "engine_cylinders    0\n",
       "highway_mpg         0\n",
       "city_mpg            0\n",
       "popularity          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num = df_num.fillna(0)\n",
    "df_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "given-bruce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.000e+02, 4.000e+00, 2.500e+01, 1.900e+01, 1.385e+03],\n",
       "       [2.410e+02, 4.000e+00, 2.900e+01, 2.200e+01, 6.170e+02],\n",
       "       [1.600e+02, 4.000e+00, 3.600e+01, 2.600e+01, 5.657e+03],\n",
       "       ...,\n",
       "       [3.320e+02, 8.000e+00, 2.300e+01, 2.000e+01, 1.624e+03],\n",
       "       [1.480e+02, 4.000e+00, 3.400e+01, 2.400e+01, 4.360e+02],\n",
       "       [2.900e+02, 6.000e+00, 2.500e+01, 1.800e+01, 1.720e+03]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = df_num.values \n",
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-guinea",
   "metadata": {},
   "source": [
    "We’re ready to apply the model to X_val to get predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "polished-kelly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.86288014, 10.37013608,  9.69868129, ..., 10.4916625 ,\n",
       "        9.57091361, 10.40022147])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = w_0 + X_val.dot(w) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-optimization",
   "metadata": {},
   "source": [
    "The y_pred array contains the predictions for the validation dataset. Now we use y_pred and\n",
    "compare it with the actual prices from y_val, using the RMSE function that we implemented\n",
    "previously: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "double-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7616530991301601"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y_val, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-geography",
   "metadata": {},
   "source": [
    "The value this code prints is 0.76, which is the number we should use for comparing models.\n",
    "In the code above we already see some duplication: Training and validation tests require\n",
    "the same preprocessing, and we wrote the same code twice. Thus, it makes sense to move\n",
    "this logic to a separate function and avoid duplicating the code.\n",
    "\n",
    "We can call this function prepare_X because it creates a matrix X from a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "tutorial-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df):\n",
    "    df_num = df[base]\n",
    "    df_num = df_num.fillna(0)\n",
    "    X = df_num.values\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-damages",
   "metadata": {},
   "source": [
    "Now the whole training and evaluation becomes simpler and looks like this:m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "enormous-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.7616530991301601\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "X_train = prepare_X(df_train)\n",
    "w_0, w = linear_regression(X_train, y_train)\n",
    "\n",
    "# apply the model to the validation dataset\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "# compute RMSE on the validation data\n",
    "print('validation:', rmse(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-waste",
   "metadata": {},
   "source": [
    "This gives us a way to check whether any model adjustments lead to improvements in the\n",
    "predictive quality of the model. As the next step, let’s add more features and check whether it\n",
    "gets lower RMSE scores. \n",
    "\n",
    "## 2.4.4 Simple feature engineering\n",
    "\n",
    "We already have a simple baseline model with simple features. To improve our model further,\n",
    "we can add more features to the model: we create others and add them to the existing\n",
    "features. As we already know, this process is called feature engineering.\n",
    "Because we have already set up the validation framework, we can easily verify whether\n",
    "adding new features improves the quality of the model. Our aim is to improve the RMSE\n",
    "calculated on the validation data.\n",
    "\n",
    "\n",
    "First, we create a new feature, “age,” from the feature “year.” Age of a car should be very\n",
    "helpful when predicting its price: intuitively, the newer the car, the more expensive it should\n",
    "be.\n",
    "\n",
    "\n",
    "Because the dataset was created in 2017 (which we can verify by checking\n",
    "df_train.year.max()), we can calculate the age by subtracting the year when the car was\n",
    "out from 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "committed-engagement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['make', 'model', 'year', 'engine_fuel_type', 'engine_hp',\n",
       "       'engine_cylinders', 'transmission_type', 'driven_wheels',\n",
       "       'number_of_doors', 'market_category', 'vehicle_size', 'vehicle_style',\n",
       "       'highway_mpg', 'city_mpg', 'popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "virgin-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['make', 'model', 'year', 'engine_fuel_type', 'engine_hp',\n",
      "       'engine_cylinders', 'transmission_type', 'driven_wheels',\n",
      "       'number_of_doors', 'market_category', 'vehicle_size', 'vehicle_style',\n",
      "       'highway_mpg', 'city_mpg', 'popularity', 'age'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>engine_fuel_type</th>\n",
       "      <th>engine_hp</th>\n",
       "      <th>engine_cylinders</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>driven_wheels</th>\n",
       "      <th>number_of_doors</th>\n",
       "      <th>market_category</th>\n",
       "      <th>vehicle_size</th>\n",
       "      <th>vehicle_style</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>popularity</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>chevrolet</td>\n",
       "      <td>cobalt</td>\n",
       "      <td>2008</td>\n",
       "      <td>regular_unleaded</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>front_wheel_drive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>compact</td>\n",
       "      <td>coupe</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>1385</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6720</th>\n",
       "      <td>toyota</td>\n",
       "      <td>matrix</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular_unleaded</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>front_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_hatchback</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>2031</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>subaru</td>\n",
       "      <td>impreza</td>\n",
       "      <td>2016</td>\n",
       "      <td>regular_unleaded</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>all_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_hatchback</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11190</th>\n",
       "      <td>volkswagen</td>\n",
       "      <td>vanagon</td>\n",
       "      <td>1991</td>\n",
       "      <td>regular_unleaded</td>\n",
       "      <td>90.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>manual</td>\n",
       "      <td>rear_wheel_drive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>large</td>\n",
       "      <td>passenger_minivan</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>873</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>ford</td>\n",
       "      <td>f-150</td>\n",
       "      <td>2017</td>\n",
       "      <td>flex-fuel_(unleaded/e85)</td>\n",
       "      <td>385.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>four_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>flex_fuel</td>\n",
       "      <td>large</td>\n",
       "      <td>crew_cab_pickup</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>5657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             make    model  year          engine_fuel_type  engine_hp  \\\n",
       "2735    chevrolet   cobalt  2008          regular_unleaded      148.0   \n",
       "6720       toyota   matrix  2012          regular_unleaded      132.0   \n",
       "5878       subaru  impreza  2016          regular_unleaded      148.0   \n",
       "11190  volkswagen  vanagon  1991          regular_unleaded       90.0   \n",
       "4554         ford    f-150  2017  flex-fuel_(unleaded/e85)      385.0   \n",
       "\n",
       "       engine_cylinders transmission_type      driven_wheels  number_of_doors  \\\n",
       "2735                4.0            manual  front_wheel_drive              2.0   \n",
       "6720                4.0         automatic  front_wheel_drive              4.0   \n",
       "5878                4.0         automatic    all_wheel_drive              4.0   \n",
       "11190               4.0            manual   rear_wheel_drive              3.0   \n",
       "4554                8.0         automatic   four_wheel_drive              4.0   \n",
       "\n",
       "      market_category vehicle_size      vehicle_style  highway_mpg  city_mpg  \\\n",
       "2735              NaN      compact              coupe           33        24   \n",
       "6720        hatchback      compact      4dr_hatchback           32        25   \n",
       "5878        hatchback      compact      4dr_hatchback           37        28   \n",
       "11190             NaN        large  passenger_minivan           18        16   \n",
       "4554        flex_fuel        large    crew_cab_pickup           21        15   \n",
       "\n",
       "       popularity  age  \n",
       "2735         1385    9  \n",
       "6720         2031    5  \n",
       "5878          640    1  \n",
       "11190         873   26  \n",
       "4554         5657    0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['age'] = 2017 - df_train.year\n",
    "print(df_train.columns)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-shore",
   "metadata": {},
   "source": [
    "This operation is an elementwise operation. We calculate the difference between 2017 and\n",
    "each element of the year series. The result is a new Pandas series containing the differences,\n",
    "which we write back to the dataframe as the age column.\n",
    "\n",
    "\n",
    "We already know that we will need to apply the same preprocessing twice: to the training\n",
    "and validation sets. Because we don’t want to repeat the feature extraction code multiple\n",
    "times, let’s put this logic into the prepare_X function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "suffering-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df):\n",
    "    # create a copy of the input parameter to prevent side effects\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create a copy of the base list with the basic features\n",
    "    features = base.copy()\n",
    "    \n",
    "    # compute the age feature\n",
    "    df['age'] = 2017 - df.year\n",
    "    \n",
    "    # append age to the list of feature names we use for the model\n",
    "    features.append('age')\n",
    "    \n",
    "    # declare a variable named, df_num. set the value to the base columns of df dataframe.\n",
    "    df_num = df[features]\n",
    "    \n",
    "    # replace Nan with zeros\n",
    "    df_num = df_num.fillna(0)\n",
    "    \n",
    "    # declare a variable named, X. set the value to df_num.values.\n",
    "    X = df_num.values\n",
    "    \n",
    "    # return X\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-farming",
   "metadata": {},
   "source": [
    "The way we implement the function this time is slightly different from the previous version.\n",
    "Let’s look at these differences. First, in A, we create a copy of the dataframe df that we pass\n",
    "in the function. Later in the code, we modify df by adding extra rows in C. This kind ofbehavior is known as a side effect: the caller of the function may not expect the function to\n",
    "change the dataframe. To prevent the unpleasant the list with the base features for the same\n",
    "reason. Later, we extend this list with new features D, but we don’t want to change the\n",
    "original list. The rest of the code is the same as previously.\n",
    "\n",
    "Let’s test if adding the feature “age” leads to any improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "removable-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.5172055461058335\n"
     ]
    }
   ],
   "source": [
    "#  prepare the training data.\n",
    "X_train = prepare_X(df_train)\n",
    "\n",
    "#  train the model using the training data.\n",
    "w_0, w = linear_regression(X_train, y_train)\n",
    "\n",
    "# prepare the validation data.\n",
    "X_val = prepare_X(df_val)\n",
    "\n",
    "# apply the model to the validation dataset.\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "# print the rmse\n",
    "print('validation:', rmse(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-programmer",
   "metadata": {},
   "source": [
    "The validation error is 0.517, which is a good improvement from 0.76 — the value we had in\n",
    "the baseline solution. Thus, we conclude that adding “age” is indeed helpful when making\n",
    "predictions.\n",
    "\n",
    "\n",
    "We can also look at the distribution of the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "accessory-prisoner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bizof\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\bizof\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\seaborn\\distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27602360b48>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABD9klEQVR4nO3dd3yV5fn48c+Vk5O9B2EkIQECYQcIqyBaceACtVrFaq22Uldt7fTX9a2tbe3X1ta2jvrVOlr31kq1DqwiCAQIkECABEISQvbe49y/P54EQ8jOOTkZ1/v1Oq+T8zz3ec5FSHI99xZjDEoppcYuD3cHoJRSyr00ESil1BiniUAppcY4TQRKKTXGaSJQSqkxztPdAfRXRESEiYuLc3cYSik1ouzcubPEGBPZ1bkRlwji4uJISUlxdxhKKTWiiMix7s5p05BSSo1xmgiUUmqM00SglFJj3IjrI1BKjR7Nzc3k5eXR0NDg7lBGDR8fH6Kjo7Hb7X1+jyYCpZTb5OXlERgYSFxcHCLi7nBGPGMMpaWl5OXlER8f3+f3adOQUsptGhoaCA8P1yTgJCJCeHh4v2tYmgiUUm6lScC5BvL91ESglFJjnPYRKKWGjWe35Tj1etcsjXXq9Xrz0Ucf8fvf/55//etfvPnmm+zfv5+77rqry7IVFRU8++yz3HrrrQDk5+dzxx138PLLLw9lyIAmgqGX8sTpx5JvGPo4VP/p/92Y1drais1m69d71q5dy9q1a7s9X1FRwUMPPXQyEUycONEtSQBc3DQkImtE5KCIZIrIaWlRRIJF5C0R2SMi6SKiv1VKqSGVnZ1NYmIi119/PfPmzeOKK66grq6OuLg4fvnLX7Jy5Upeeukl/vOf/7B8+XIWLlzIlVdeSU1NDQDvvPMOiYmJrFy5kldfffXkdZ988kluv/12AAoLC7nsssuYP38+8+fPZ8uWLdx1111kZWWRlJTED37wA7Kzs5kzZw5gdaLfcMMNzJ07lwULFrBp06aT17z88stZs2YNCQkJ/PCHP3TK98BliUBEbMCDwAXALGC9iMzqVOw2YL8xZj5wFvAHEfFyVUxKKdWVgwcPsmHDBvbu3UtQUBAPPfQQYI3J37x5M+eccw733HMP77//Prt27SI5OZn777+fhoYGbrrpJt566y0++eQTCgoKurz+HXfcwZlnnsmePXvYtWsXs2fP5t5772Xq1KmkpqZy3333nVL+wQcfBGDfvn0899xzXH/99SdHAqWmpvLCCy+wb98+XnjhBXJzcwf973dljWAJkGmMOWKMaQKeB9Z1KmOAQLG6uQOAMqDFhTEppdRpYmJiWLFiBQDXXnstmzdvBuCqq64C4LPPPmP//v2sWLGCpKQknnrqKY4dO0ZGRgbx8fEkJCQgIlx77bVdXv/DDz/klltuAcBmsxEcHNxjPJs3b+a6664DIDExkcmTJ3Po0CEAVq9eTXBwMD4+PsyaNYtjx7pdS67PXNlHMAnomKrygKWdyvwVeBPIBwKBq4wxjs4XEpENwAaA2Nih7fxRSo1+nYdctr/29/cHrIla5557Ls8999wp5VJTU10y/NUY0+05b2/vk1/bbDZaWgZ/7+zKGkFX353O/7rzgVRgIpAE/FVEgk57kzGPGmOSjTHJkZFdLqetlFIDlpOTw9atWwF47rnnWLly5Snnly1bxqeffkpmZiYAdXV1HDp0iMTERI4ePUpWVtbJ93Zl9erVPPzww4DV8VxVVUVgYCDV1dVdll+1ahXPPPMMAIcOHSInJ4cZM2YM/h/aDVfWCPKAmA6vo7Hu/Du6AbjXWOkvU0SOAonAdhfGpZQapoZ6uGe7mTNn8tRTT/HNb36ThIQEbrnlFv7yl7+cPB8ZGcmTTz7J+vXraWxsBOCee+5h+vTpPProo1x00UVERESwcuVK0tLSTrv+Aw88wIYNG3j88cex2Ww8/PDDLF++nBUrVjBnzhwuuOACbrvttpPlb731Vm6++Wbmzp2Lp6cnTz755Ck1AWeTnqogg7qwiCdwCFgNHAd2ANcYY9I7lHkYKDTG/EJEooBdwHxjTEl3101OTjYjemMaHYI4MnT1/9QV/b8blAMHDjBz5ky3xpCdnc3FF1/c5R/wkaqr76uI7DTGJHdV3mU1AmNMi4jcDrwL2IC/G2PSReTmtvOPAL8CnhSRfVhNST/qKQkopZRyPpdOKDPGbAQ2djr2SIev84HzXBmDUkr1JC4ublTVBgZC1xpSSqkxThOBUkqNcZoIlFJqjNNEoJRSY5yuPqqUGj76Omy3r3oZ3tt5KWhXef3115k+fTqzZnVebm140BqBUmrMal8Kuq+MMTgcp62C06vXX3+d/fv39/t9Q0UTgVJqzOq4FPSdd97J6tWrWbhwIXPnzuWNN94ArAlnM2fO5NZbb2XhwoXk5ubyq1/9isTERM4991zWr1/P73//ewCysrJYs2YNixYt4owzziAjI4MtW7bw5ptv8oMf/ICkpKSTy1EMJ9o0pJQas+69917S0tJITU2lpaWFuro6goKCKCkpYdmyZSc3ljl48CBPPPEEDz30ECkpKbzyyivs3r2blpYWFi5cyKJFiwDYsGEDjzzyCAkJCWzbto1bb72VDz/8kLVr13LxxRdzxRVXuPOf2y1NBEophdXs8+Mf/5iPP/4YDw8Pjh8/TmFhIQCTJ09m2bJlgLVE9Lp16/D19QXgkksuAaCmpoYtW7Zw5ZVXnrxm+7pEw50mAqWUAp555hmKi4vZuXMndruduLi4k5vBtC9HDd0vEe1wOAgJCSE1NXUownUq7SNQSo1ZHZeCrqysZNy4cdjtdjZt2tTthi8rV67krbfeoqGhgZqaGt5++20AgoKCiI+P56WXXgKshLFnz57TPmc40hqBUoOhq8k61xB/78LDw08uBb148WIyMjJITk4mKSmJxMTELt+zePFi1q5dy/z585k8eTLJyckndxx75plnuOWWW7jnnntobm7m6quvZv78+Vx99dXcdNNN/PnPf+bll19m6tSpQ/nP7JXLlqF2FV2GWg2JwYxn1//PPhsOy1APRE1NDQEBAdTV1bFq1SoeffRRFi5c6O6wTho2y1ArNWZpsh/1NmzYwP79+2loaOD6668fVklgIDQRKKVUPz377LPuDsGptLNYKeVWI615ergbyPdTE4FSym18fHwoLS3VZOAkxhhKS0vx8fHp1/tc2jQkImuAB7C2qnzMGHNvp/M/AL7SIZaZQKQxpsyVcSmlhofo6Gjy8vIoLi52dyijho+PD9HR0f16j8sSgYjYgAeBc4E8YIeIvGmMObnykjHmPuC+tvKXAHdqElBq7LDb7cTHx7s7jDHPlTWCJUCmMeYIgIg8D6wDuluCbz3wnAvjUcp56ssh5zPw8ofIGRAQ5e6IlBowVyaCSUBuh9d5wNKuCoqIH7AGuL2b8xuADQCxsbHOjVKp/sr+BPa/Do5W67WHDeZfA5MWuTUspQbKlYlAujjWXY/QJcCn3TULGWMeBR4Fa0KZc8JTagCKMyDtVYhMhHlfBmMg9RnY/Q/rvCYDNQK5ctRQHhDT4XU0kN9N2avRZiE13DVWw66nIXA8LPoa+IaCXxgsvQVC4yDtFWiocneUSvWbKxPBDiBBROJFxAvrj/2bnQuJSDBwJvCGC2NRavCyNkFzPSy8Hjy9Pz9u84T566G1yUoGSo0wLksExpgWrDb/d4EDwIvGmHQRuVlEbu5Q9DLgP8aYWlfFotSgNdbAsc0waaFVI+gsIAqmnQsFe6Ai9/TzSg1jLp1HYIzZCGzsdOyRTq+fBJ50ZRxKDdrRj6C1mT2+S2g42vUI56XxZ8KRTZD1PizStYXUyKEzi5XqjaMVcrdD1GwavCO7L2f3gbgz4MReqCkcuviUGiRNBEr1puQQNFZB9OLey8avsoaTZm92fVxKOYmuPqoU9Lz/wPEUsPvCuNmQ08uoIO9AGD8Xju+EmeusjmSlhjmtESjVk5ZGKNgHE5L6/kc9egk010FhmktDU8pZNBEo1ZPiDGtY6MR+bDwSOQN8giFvu+viUsqJNBEo1ZOiA+DpA2FT+v4e8YBJydZ7m2pcF5tSTqKJQKnuGGP9MY+cYXUA98fEJMBAgTYPqeFPE4FS3anOh8ZKGDer/+8NiraWoCjY6/y4lHIyTQRKdaeobcX0yMT+v1cExs+DkoPQ3ODcuJRyMk0ESnWn+CAETbI6fgdiwnxrMlpRunPjUsrJNBEo1ZXWFig/BuHTBn6N0DjwCoDC7vZiUmp40ESgVFcqc8DRDOFTB34N8bCalYozPt/ERqlhSBOBUl0pzbKe+zNstCvjZkJzLeSnDjokpVxFE4FSXSnLspab9goY3HUiZwACme85JSylXEETgVKdOVqh7CiEDaJZqJ1XAITEQOb7g7+WUi6iiUCpzqryobVx8M1C7SJnQl4K1Fc453pKOZlLE4GIrBGRgyKSKSJ3dVPmLBFJFZF0EfmvK+NRqk8qjlnPofHOuV5EAmDgw19Zq5y2P5QaJlyWCETEBjwIXADMAtaLyKxOZUKAh4C1xpjZwJWuikepPqs4ZjXp+IY653ohk8HDE0oznXM9pZzMlYulLwEyjTFHAETkeWAd0HFQ9TXAq8aYHABjTJEL41GqT+qLjtLgPZFD2eXOuaDNbs0p0ESghilXNg1NAjru4p3Xdqyj6UCoiHwkIjtF5KtdXUhENohIioikFBcXuyhcpYDmenybSqjxnejc64ZPg8rj1j4FSg0zrkwE0sUx0+m1J7AIuAg4H/iZiEw/7U3GPGqMSTbGJEdG9rBnrFKDVZEDQK1v53uWQQqfBhgoO+Lc6yrlBK5MBHlATIfX0UB+F2XeMcbUGmNKgI+B+S6MSametSUCp9cI2vsJSg4797pKOYErE8EOIEFE4kXEC7gaeLNTmTeAM0TEU0T8gKXAARfGpFTPKo5R7xVOq83Xude12a1RSO0zlpUaRlyWCIwxLcDtwLtYf9xfNMaki8jNInJzW5kDwDvAXmA78JgxRnfyUO5TdZxan/GuuXb4VKg6Dk3aT6CGF1eOGsIYsxHY2OnYI51e3wfc58o4lOqTpjqoL6cusB/7E/dHeALwjtVPMH6Oaz5DqQHQmcVKtas6DkCdT5Rrrh8SCx52KNV+AjW8aCJQql2VNZahzlVNQyfnE2g/gRpeNBEo1a7qOHgF0Gwf5IqjPWnvJ9D5BGoYcWkfgVIjSlW+tTVlH9S1erCzIoDjDV5M9W9gflBt3z4jbApgrN3PlBomNBEoBdbS0zUnIO7MXoserfPmt4djqGz5/Ncn2qeRv4XXMjukpec3h8QCAuXZg4tXKSfSpiGlAGoKrWQQ1PNEspx6b+4+GIunGH4x/Rj/WHCQ70/No7bVgys3hbC7tJd7K08f6zPKjzoxeKUGRxOBUnCyo5jg7puGWg08nD0eLw/DrxKPMTOwHi8Pw+KQGn47M5tIHwdf2xxCVrWt588KjbeahnQfYzVMaCJQCqwOXA9P8B/XbZGNRWEcqfPlhphCwr1ObQIKtbfyz1UV2ARu+yyIhp7+xofGWRvfFO3voZBSQ0cTgVJgJYLA8eDR9d18o0N440QYSUE1LAut7rJMjL+DPyyuIqPSzm/39jDyKKxtw5vcbYONWimn0ESglDFWIuhhxNBHJcFUt3py2YRSpKt1ddt8cUITX5tWx9NZvuzsrr/ANwy8gyB3+yADV8o5NBEoVV0ATbXddhS3GvhXYRjT/etIDKjv9XLfn13LeF8HP9kVRLOjiwIiVvOQ1gjUMKGJQKmCfdZzUHSXp/dV+VPU5MVFUX3bsSzAbrh7QTUZlZ48ftiv60Kh8dYQ0urCAQSslHNpIlCqsD0RTOjy9MdlQfjbWlkUXNPnS543sYnzJjbyp/3+5NZ28WsWGmc952nzkHI/TQRKFaRZG9XbT797r2/1YEd5IMtDq7B7dN5gr2e/SKrGJoa7UwNPPxkcDTYvbR5Sw4ImAqUK07vtH9heEUCT8eCM8Kp+X3ain4NbE+t4/4Q3W4rsp5602WFCEuTuGEDASjmXJgI1trU0QVkWBHbdLLS9PJBwezMz/HvvJO7K1xPqmOTXyq/2BNDauUIRswTyd0NL44CurZSzuDQRiMgaETkoIpkiclcX588SkUoRSW17/NyV8Sh1mtJMcLRAwOlLTzc5hL1V/iSH1PQ4ZLQnPja4a24NByrtvJztc+rJmCXWxLL2zmql3MRliUBEbMCDwAXALGC9iMzqougnxpiktscvXRWPUl0qbtsiO/D0RLCvyo8m40FySNcTyPrq4uhGFoU3cV+aPzXNHTJK9BLrWecTKDdzZY1gCZBpjDlijGkCngfWufDzlOq/ogwQDwg4fWmJnZWB+Hq0MitgcHsHiMDP5tdQ0mjj4YMdOqQPvQM+IbDvRUh5YlCfodRguHIZ6klAbofXecDSLsotF5E9QD7wfWNMeucCIrIB2AAQGxvrglDVmFWcYQ3ltHmdctgY2FXpz/zgWjz7eLu07WhZt+eWxodxaWwD/3fIj/Xx9UT7t800C42jsfgIqUfLyGrNOe191yzVn3fleq6sEXTVqtq5u2wXMNkYMx/4C/B6VxcyxjxqjEk2xiRHRkY6N0o1thVnQOTM0w7nNnhT3mwnqa8bzvTBD+fU4CHwv2kd1iEKjcO7uRJ78+Can5QajD4lAhF5RUQuEpH+JI48IKbD62isu/6TjDFVxpiatq83AnYRiejHZyg1cC2N1v7B4xJPO7W3ymrCmevERDDRz8E3Eup4M9eH/RVtlfG2iWUB9XlO+xyl+quvf9gfBq4BDovIvSJy+m/O6XYACSISLyJewNXAmx0LiMh4EWs8hogsaYuntM/RKzUYpZlgWrusEeyr8meidyMRXr3sONZPN02vI8ju4A/p/taBoGgcYiOgThOBcp8+JQJjzPvGmK8AC4Fs4D0R2SIiN4iIvZv3tAC3A+8CB4AXjTHpInKziNzcVuwKIK2tj+DPwNXGmP5N31RqoIraRgx1qhE0O4QDNX7MDXL+BvPBXoZvzqjjgxPe1uqkNk9qfcZrjUC5VZ+bekQkHPga8A1gN/AAVmJ4r7v3GGM2GmOmG2OmGmN+3XbsEWPMI21f/9UYM9sYM98Ys8wYs2UQ/xal+qe4bcRQeMIphw/X+tDo8HBqs1BHN0yrI8Lbwe/b+gpqfKMJqD+BOJpd8nlK9aavfQSvAp8AfsAlxpi1xpgXjDHfAnrYgUOpYazoAIRNAfupE73Sqv0RDLMDnV8jAPDzhNsSa9la7MWnhXZq/KLxMC2EVh9yyecp1Zu+1ggeM8bMMsb81hhzAkBEvAGMMckui04pVyrOgMjTu7sOVPsS59eIn62rzQSc45op9Uz0beWP+/2p8bWWv46o2OOyz1OqJ31NBPd0cWyrMwNRakg1N0DZERh3akdxYyscrvUd9CSy3njb4Jsz6kgp9WJPQxRNnoFElGsiUO7RYyJoG9WzCPAVkQUisrDtcRZWM5FSI1PpYTCO02oEe8rsNBsPZrqoWaijL8fVE+7t4PXCCGp8JxFesdfln6lUV3qbWXw+VgdxNHB/h+PVwI9dFJNSrleUYT13qhFsK7EjGGa6uEYA4OsJN06r4770AHInxjG/+h28G0tp9A53+Wcr1VGPicAY8xTwlIh8yRjzyhDFpJTrFR8AsUH4tFMOby/2Isa3kQBP5/YPdLf8RKJnBV4yjY31s5jPO0RU7OV41Bed+tlK9aa3pqFr276ME5Hvdn4MQXxKuUZRBoRPBU/vk4daHLCrzLNPG9Q7S4CngzPCK3mmYg4O8dQOY+UWvXUWt01/JAAI7OKh1MjUxYihg1We1LZ4MGMIEwHABePKqTE+5HhNJUL7CZQb9NY09Le257uHJhylhkBzA5QfhblXnHJ4Z4k1SX6Gv+v7BzqK8W1iVkAdW5umcGXLJsTRgvFw5cLASp2qrxPK/ldEgkTELiIfiEhJh2YjpUaWkyOGZpxyOKXUTpRPq9PXF+qL1ZEVbG2cgmdrA8E1mUP++Wps6+s8gvOMMVXAxVirik4HfuCyqJRypfYRQ50Wm9tZaic5onnA21IOxpKQavbbrKYqnU+ghlpfE0H7wnIXAs8ZY7rfgUOp4a4447QRQwX1Hhyvs7Eo3D3r/Xh5GCJjEigxQYSWpbolBjV29TURvCUiGUAy8IGIRAINrgtLKRcqbh8x9PmuZDtLrXsddyUCgIWxYexyJBBcmuq2GNTY1NdlqO8ClgPJxphmoBbdf1iNVEUHThsxlFJix8dmmBUy9P0D7SaG+HDInsi45jy8mircFocae/qz49hM4CoR+SrWPgLnuSYkpVyofcRQp0Sws9TO/NBm7K7cvLUXIkJD1CIAfAp2ui8QNeb0ddTQP4DfAyuBxW0PXXVUjTztI4Y6bEZT19RCeoUnyRHu3w8gcMoSWowHHvkp7g5FjSF9HaycDMzq7+5hIrIGawMbG9ZS1vd2U24x8BlwlTHm5f58hlL9UnzQeu5QI9iTW0mrEZLd2D/QLjAomCyPyYyr3EuFu4NRY0ZfK8JpwPj+XFhEbMCDwAXALGC9iMzqptzvsLa0VMq1ik5fY2jnMWsQ3IJhkAgA8gPnMqP1MNV1Oh5DDY2+JoIIYL+IvCsib7Y/ennPEiDTGHPEGNMEPE/XHczfAl4BivoctVIDVZxh7UrWYY2hncfKSQhqIcRreGyX3Ri1iECpp+zYPneHosaIvjYN/WIA154E5HZ4nQcs7VhARCYBlwFnY/U7KOVaxRmnLD3tcBh2HivnwgnDozYA0DxhERwG7xM7gS+5Oxw1BvR1+Oh/gWzA3vb1DmBXL2/ran5m51uuPwE/Msa09nghkQ0ikiIiKcXFxX0JWanTtTRau5J1mFGcVVxDVUOLW+cPdFbtF0u1RxAxdWlU1DW5Oxw1BvSpRiAiNwEbgDBgKtbd/iPA6h7elgfEdHgdDeR3KpMMPC/WnP4I4EIRaTHGvN6xkDHmUeBRgOTk5OFRf1cjT8npawylHCsHGBYdxVNzXjr5dZX3BJJaMnn/QBFXLIp2Y1RqLOhrH8FtwAqgCsAYcxgY18t7dgAJIhIvIl7A1cAp/QrGmHhjTJwxJg54Gbi1cxJQymmKT9+VLCW7nHB/L+ICeqyUDrnWgIkkeBzn4z2H3R2KGgP62kfQaIxpartzR0Q8Ob2Z5xTGmBYRuR1rNJAN+LsxJl1Ebm47/8jAw1ZqANrXGMreArnbAdh1KIyFwa1uWWiuJ7V+k6zno9uobTwDf29dllq5Tl9/uv4rIj/G2sT+XOBW4K3e3mSM2Qhs7HSsywRgjPlaH2NRamDaRwzZrB/7kgbhaI0nV8UPv2GaNb6TMAhzzUE+zSzhvNn9Gr2tVL/0tWnoLqAY2Ad8E+uP+09dFZRSLlGUcWr/QKm16NziiOHXIeuweVPnHUmyLYuPDukACeVafaoRGGMcIvI68LoxRn8q1cjTPmJo9qUnD+0osePtYZgb6r6F5npS4xfNwuaD3JVRiDFzkOHWfqVGjR4TgVg/ef8D3I41HFREpBX4izHml0MQn1LOUZoJptVaWqKxGrASQVJYM15uXGiuJzW+0USV78K76gh/ej+WqCCfU85fszTWTZGp0aa3X4HvYI0WWmyMCTfGhGFNClshIne6OjilnKZ9xFDbGkO1LUJ6hSeLh8FCc92p8bOGjS70OMyhwmo3R6NGs94SwVeB9caYo+0HjDFHgGvbzik1MhRlgHicXGNod6knrUaGdSJo8AqnyTOQL3gd0USgXKq3RGA3xpR0PtjWT2DvorxSw1P7iCG71byyvcQLDwwLh8FEsm6JUBIyj2RbJtmldTS2DK+5Dmr06C0R9DScYvgNtVCqO8UZpyw9nVJiZ2ZIC4H24T1RvSRkPtEtx/B11HKkuNbd4ahRqrdEMF9Eqrp4VANzhyJApQatpRFKs04mgmYH7C6zD+tmoXbFoUkIhiX2LA5q85BykR4TgTHGZowJ6uIRaIzRpiE1MpRmWSOG2paWSK/wpL51ePcPtCsJmY9DPDnPP4tDhdX0c28opfpkmA6cU8qJig9Yz22TyXaUWPcwi4dz/0CbVk8/yoJmsVgOUFHXTHF1o7tDUqOQJgI1+hWmg4cnREwHYEeJF5P9Wxjn63BzYH1TFLaIuIYMvGnS0UPKJTQRqNGvMN1KAp7eOByGHSX2YbFRfV8VhSVjM82c5X+MQ4U17g5HjUKaCNToV5gOUbMB2H+iivImD1aOGzmD3opDkzAI5/lncrS0lqaWkVGTUSOHJgI1utVXQGXuyUTwaaY1LWbFuJFTI2i2B1EeOIOFZj+tDkNWsdYKlHNpIlCjW9F+6zlqDgCbM0tICBo5/QPtisKSialNx9/m0H4C5XSaCNToVphuPUfNpqG5lR3ZZawYQc1C7YrCFuHpaOD80HwdRqqcTrc9UqNbYRr4hkLgBHYdKaWh2cHKcU1sO1rm7sj6pTh0IQBf9D3MqyXROoxUOZVLawQiskZEDopIpojc1cX5dSKyV0RSRSRFRFa6Mh41BhWmW81CInyaWYLNQ1gaOXL6B9o1eodRETCVea1WU5c2DylnclkiEBEb8CBwATALWC8iszoV+wCYb4xJAm4EHnNVPGoMcjigcH+HjuJSkmJChv36Qt0pDl3ExMpUJgR46nITyqlcWSNYAmQaY44YY5qA54F1HQsYY2rM542d/sDI/A1Vw1NFNjTXQtRsKuub2ZtXwYppEe6OasCKwhZhb63l3LBCskvqqGkcnjurqZHHlYlgEpDb4XVe27FTiMhlIpIBvI1VKziNiGxoazpKKS7WnTJVH3XoKP7sSCkOAytHWCKYmvPSyYdPQyEAZ9nTaTWGLZmnrRCv1IC4MhF0tcHqaXf8xpjXjDGJwKXAr7q6kDHmUWNMsjEmOTIy0rlRqtGrMB0QiJzJp5kl+HnZSIoJcXdUA9biGUCtdxSz6nfi7enBpoN6U6Scw5WJIA+I6fA6GsjvrrAx5mNgqoiMrFs2NXwVpkH4VIzdl48OFrNsSjheniN7xHRVQDyR5anMjvTko4NFOoxUOYUrfyt2AAkiEi8iXsDVwJsdC4jINBGRtq8XAl5AqQtjUmNJYTp4+ZO56R/klNWx2i8TUp5wd1SDUhkwBZtp5vyAo5yobNBOY+UULksExpgW4HbgXeAA8KIxJl1EbhaRm9uKfQlIE5FUrBFGVxm9xVHO0FgDZUchcCLv53sDsHrCyJtI1lm132Raxc4y9iIC/0kvdHdIahRw6YQyY8xGYGOnY490+Pp3wO9cGYMao4ozAMOh+iBey/cg3q+BYwUlHHN3XIPk8LBTHLqA2IptLIq9hnfSCrhjdYK7w1Ij3MhuMFWqOwV7ASj0nMDhWl8WBY+ehdoKIpYTWn2IyxLs7D9RRW5ZnbtDUiOcJgI1OuWn0mgP4uOaaAzC4pDR05ZeELEcgDV+GQC8m17gznDUKKCJQI1O+bspC5rNZxXBjPduYrLv6FmbpzwokUZ7EOFFW5k1IYh30jQRqMHRRKBGn+YGKNpPQcBM0qv9WBZahXQ1q2WEMmKjMHwpZG1izewoduaUU1TV4O6w1AimiUCNPkXp4Ghhd0s8DoRloaOnWajdifDlUJ3P2knVGAP/2a+jh9TAaSJQo0/+bgDeLZ/AeO8m4kZRs1C7E5HWQr2TSzczJcJf+wnUoGgiUKNP/m5afcLYVubLmeGVo6pZqF2d7wQYPxc59G/OnzOerVmllNeO/HkSyj00EajRJz+VXN8ZCMKq8Ep3R+N07YvQETgRcj5jnfmQFofhX/tOuDs0NUJpIlCjS3M9pugAH1VHMyXSnwivUbxUc9QcwDCjYR8zogJ5bVeeuyNSI5QmAjW6FKQhppVP62JInhzm7mhcKzgavIORwjQuWziJXTkVZJfUujsqNQJpIlCjS1tH8XG/RGZPCnJzMC4mAuNnQ3EG6+aEIQKvaq1ADYAmAjWqVB/dQbEJ5rylSXh6jIEf76g50NrEhLIUViVE8kJKLi2tDndHpUaYMfCbosaS2qM7SDNTuGbpZHeHMjTCE8DmBQc3cs3SWAqrGnXDGtVvmgjUqJFfXEJkQzZmwgLGBfm4O5yhYbNDZCIc/DerZ0QSFeTNs9tG+hqraqhpIlCjxr/efRebGOYvPcvdoQytqDlQfQLPwlSuWhzLR4eKOaqdxqofNBGoUSGzqJrijK0AhCcsdXM0QyxqNnh4QvrrXLssFruHB098etTdUakRxKWJQETWiMhBEckUkbu6OP8VEdnb9tgiIvNdGY8anYwx3P3WfhZ7HqY1KBYCx7s7pKHl5Q9Tz4b01xgX4M26pIm8lJJHRZ3ONFZ947JEICI2rO0nLwBmAetFZFanYkeBM40x84BfAY+6Kh41er219wSfHC5mhWcGtoBIa1/ilCes2bdjxezLoTIX8nbw9TPiqW9u5akt2leg+saVW1UuATKNMUcAROR5YB2wv72AMWZLh/KfAdEujEcNY89uy+n23DVLY7s9l19Rz09f28d5k5rwL62AsDjnBzcSJF5ojR5Ke5XEC+7l3FlR/P3To9y4Mo5AH7u7o1PDnCubhiYBuR1e57Ud687XgX93dUJENohIioikFBfr0DhlaWhu5bZnd9HiMPw6uW27xtB49wblLj7BkHAepL0CrS3ccXYClfXNPLUl292RqRHAlYmgqzUfTZcFRb6IlQh+1NV5Y8yjxphkY0xyZGSkE0NUI1Vzq4PvPJ9Kam4F9395PpHle6w74sAJ7g7NfeZfDbVFkPUhc6ODOWfmOP728RHKdFVS1QtXJoI8IKbD62ggv3MhEZkHPAasM8aUujAeNUrUN7XyzX/s5J30An560SzWzJkAudsgZDJ42NwdnvsknA++YbDnWQB+tCaRuqZWHnj/kJsDU8OdK/sIdgAJIhIPHAeuBq7pWEBEYoFXgeuMMaP/p/XEXtj5JNSVgF84TPkihMa5O6oRZU9uBd99MZWjJbXcc+kcrl02GRqqoGAfTDvH3eENqW1Hy05+ndVq9bEsGnc+0w68gq2+goSoEK5ZEss/t+Vw3fI4po0LAAbeH6NGL5fVCIwxLcDtwLvAAeBFY0y6iNwsIje3Ffs5EA48JCKpIpLiqnjcbtfT8NhqKDlsDfcrOQyf/gmOfOTuyIa95lYHn2aWcPuzu1j34KfUNrby9I1LrSQAVm3AOCB8mnsDHQaOTFqHzdEE+6wRU985JwE/u43fbjzg5sjUcObKGgHGmI3Axk7HHunw9TeAb7gyhmEhaxO8eQdMOQumrgbvAGhphNRnYf/rsO1vsPSb7o5yWGludZBZVEN6fiX/+24GFXXN+HnZuO2LU9mwairBvh1GwmRvBg+71q6A8uDZlAbNIjzlCVj8DcIDvLnt7Gnc++8MNh8uYWVChLtDVMOQSxOBAqoL4JVvWOvBXP0M7H3ROu7pDQu/CjufgHd/DDFLYOIC98baR65qWmhqcXDgRBXp+ZUcKqyhqdWBj92DC+dM4LzZ4zlzeiS+Xl30AWRvhkmLrM5iRWbslYSn3Q252yF2KV/7QhzPbDvG/7yZxsZvn+Hu8NQwpInA1d6/Gxqr4YaNVpNQRx42mL8etv4VXt0A3/wY7L7uidON0vMreSP1OKm5FTS2OAj09mRBbAizJgYxJSKA65Z3v5Loi59mcEX+bvZPuZHaDm3mY9mxCRey9ND9kPI4xC7Fx27jnkvncv3ft/PgpizGj5UF+VSfaSJwpYJ9sOc5+MK3IHJG12W8/GHdg/DPy2Hrg7Dq+0Mb4xDqXJM4UVnPe/sLySioxtNDmDspmOS4MCaH++HRxx3nI8pT8TCtFIUl41933BVhjzgtnn6QtN6aYX3uLyHQqk1dmjSRhz/K5NazphGlyUB1oIvOudL7v7Am+pzx3Z7LTVsNiRfD5j9CdeGQhOZOpTWNvLAjh79+mEl2aS3nzYri/10wkyuTY4iP8O9zEgCIKttGq3hSEpLkuoBHoqU3g6MFdjx28tDPLp5FgLcnr+0+jsN0OaVHjVGaCFylYB9kvg8r7gDf0N7Ln/tLaGmAj37r+tjcpKq+mddTj/PH9w+x/0QVq6ZH8oPzEjlrxriu2/77YELJVkpCk6y7YPW58KmQeBHseByarFnX4QHe/PSiWeSU1bFdm9FUB5oIXGXrQ2D3g+Qb+1Y+fCosugF2/xMqcnsvP4KU1jRy778z+MN7B0nJLmNxXBjfO28G588eP+AEAEBtCWFVBygIX+68YEeT5bdBfZn1M9Xm8oWTmBYZwLvpBVTWN7sxODWcaCJwheoCaxz3gmv7Vhtot+LbgIEtf3ZZaEPpWGktP3s9jS/c+yF/+ziL2RODufOc6axLmkSQMxZCa5uDcSLiC4O/1mgUu9x6fPona7gyICKsS5qIwxje3JOP0SYihSYC19j5FDiarXba/giJsUYR7Xp6xPYVnKis5/HNR/nSw1s4876PeGFHLpcmTeK9O1fx5eQYwgO8nfdhWR/SaA+iPHim8645mojAmT+EquOQ+szJw+EB3pwzM4oDJ6rYm1fpxgDVcKGjhpzN0Qq7/2EtHxE+tf/vX3mn9Uu79a9w3q+cH58LVNY3k3a8kn3HK/nxa/sASBwfyPfOnc6XF8ecHKGy/Wj5gK7f5bwFY7j0wHsUhy/DyBheX6gbJ79nZirnhczD9/3f8VbTGThsViJeMS2CtOOVvLknnymR/rpU9RinNQJnO7LJ2iBk4VcH9v7wqdYmIzseh7rh26HX2NzKjqNlPPLfLH73TgZv7ztBc6uD7583nQ+/dybvfGcV31qd4LJhiqFVGfg1FnEicqVLrj9qiLBn+h34NxQwPef5k4c9RPjSomiaWx28nqpNRGOd1gicbdfT1oJyiRcN/BpnfA/SXobPHoazf+K82Jwgt6yO13bnsSe3kqZWB+MCvTlvVhRzJgYTEeg9ZIuWTSr6CINwPHLVkHzecNfVbmxZsVcCUBi+lPyIFczOepSs6EtptgcDMC7Qh3NnRfHvtAL25FWQFNOP/iw1qmiNwJnqyiBjI8y7ylpCYqCiZlnzCrb/zZqVPAwUVjXw09f3cfYfPmJ3TgVzo4O5+cypfHt1AmfNGEdEoBPb/vtgUvF/KQ2ZS6N3+JB+7kiVOuNOvJqrmXf44VOOr5gWQWyYH2/tOUFVg44iGqs0EThT+qtWJ/H89YO/1srvQkOlNTvUjYwxPLPtGGf//iOe357LVYtj+P55M/jSwmhiw/yQfkz+chbfhiLCK9PJG3fWkH/2SFURNIPDsV8m4dhzhFQdPHncQ4QrFrY1Ee0+rk1EY5QmAmfa8zyMmw3j5w7+WtGLIH6VtexE29C/ofbgpkzOvf9jfvJaGuODffj26gRmTQgmyNe9HYsTiz8G4Pi4M90ax0izZ/q3aLIHszj9HmvZ7jYRbc17GQXVvLZbl+kYizQROEtJJuTtgPlXWcP2nGHld6GmwFqvaAi11wIe+OAwOeV1rEuayI0r4p079HMQYgveo9ovhsqABHeHMqI024M5Pm4VkRWpfCH1h0zNeelk38IXpkUwOcyPX7yZTmFVg5sjVUNNE4Gz7H0BxAPmftl515xyFkxIgk8fsIalDoG88jqufXwbP3ktjZhQX759dgJL48Pd0gTUFe+mcqJKt5Ez/nznJdwxpCR4HhUB04gp/ADvxs9HpbWPImpqdfD9l/bgcGgT0Vji0lFDIrIGeACwAY8ZY+7tdD4ReAJYCPzEGPN7V8bjMg4H7H0e4s+EICduni5iLVj34letDWzmfMl51+7E4TA8uz3n5E5Wv75sDhj6nQB62qvAGaILP8DDtHJswvku/ZxRS4SjEy9mbuYjJOS9Qnr8DSdPRQR487OLZ/GT19J4bPMRNqwawDwYNSK5rEYgIjbgQeACYBawXkRmdSpWBtwBjMwE0C73M6jIgflXO//aiZdAeAJ8cr+VcFwgo6CKK/+2lZ++nkZSbAjvfGcVX1k6edjUAjqKPfEuVX6TqQjsZllv1asmexBZk9bh33CCyQX/OeXcNUtiWTN7PPe9e5B9Out4zHBl09ASINMYc8QY0wQ8D6zrWMAYU2SM2QGM7HFre54Du7815NPZPDysPQoK0yDjLadeury2iV+/vZ+L/7yZI8U13HfFPP759aXEhA3PlTx9GoqJKt1OzoTztFlokCqCZpAfvpyo8hQm53++m6yIcO+X5hIR4M23nttFTWOLG6NUQ8WViWAS0HEZzby2Y6NLcz2kvwEzL7H2InaFuVdatYJNv3VKraCyvpn73zvEGf+7icc2H+XyhZP44HtncWVyzLCsBbSLz/8XHjg4Ommtu0MZFfKizqbaL4Ylab8gqObIyeMhfl786aokcsrq+IH2F4wJruwj6OovyoB+okRkA7ABIDZ2aGau9tn+N6GxEhZ8ZeDX6DBXYNvRspMzQjuaHH0TK/b8ENJegXmnn++NMYaUY+U8tz2HjftO0NDs4MK54/nOOdOZHhU48NiHijFMyXud4pAkqv3j3B3NiNDVbOOOjNjIjP4SidlPc1bKLby37B+A9fu1dEo4P75wJve8fYC/fJjJt8/REVqjmSsTQR4Q0+F1NJA/kAsZYx4FHgVITk4eXrcnu/8BoXEw2bVr3hybcD4rCv8JH/zSqn3Ye17DxxjD8Yp69uRW8vGhYv57qJiCqgYCvD25fGE01y6dzKyJQS6N2ZnCK/cRXHuEbXN+4e5QRpUmexAfJT/EOdtu5KyUW2Hpe9auesDXV8azP7+KP75/iBnjA1kzZ7ybo1Wu4spEsANIEJF44DhwNXCNCz9v6JUdhexP4Is/tdryXUk84Lx74Om1lH74F7ITv055bTPldU1U1FnP5XVNlNc2U1LTyKHCaqoarPbdIB9PzkiI5IuJ47hgznj8vUfeElNTc1+hxebLsfHWaKHe7nZV34VV7icr+jKmH3sO/m813LIFPL0QEX5z+VyySmr57oupTAxZxrzoEHeHq1zAZX8RjDEtInI78C7W8NG/G2PSReTmtvOPiMh4IAUIAhwi8h1gljGmylVxOVXqM4BAkvPzmzGG4ppGskvqyC6t5URlPXfXNfOQLGDJlj9wy6YJFPH5ImGeHkKInxehfnZC/b24eP5EZk0IYvbEINKOV2HzEJpaHLyRemqlbKgWiRsMr6YK4vLf5uiktbTYXdQPM8ZVBkzl6KS1TD3+ujVc+conwe6Dj93Go9ct4opHtvDVv2/n+Q3LSBw/cmqSqm9kpK0tkpycbFJSUtwdhjXB609zYdxMuPaVvr+vl7WDNmVW8FLLKlKOlVNUbS0t4e/tSUyoLyF+XiR4FvHz3K9zMOQMNs74DX5envh52fD29BjWHb2DMTPrcRYc+hNvr3yVykCrrVprBK6x1OOAtbvelC/C1c+ClzWCLLesjisf2UqLw8EL31zO1EhNyCONiOw0xiR3dW7ktREMF1mbrJ2fzv+NUy7X2ApPZfrywP5walsLiA3zY+38iUyLDCA8wKvDH/mJHPC+iXmHH6SkOYX80NG9DLM4mpme8zwFYUtOJgHlOtscM3HMvYel+35OycMX8dGiB2mxB3DN0lieuWkpV/1tK9f832c88bUlI6qPSfVMl5gYqN3/AN8wmHHBoC+1v8KTSz4I4zf7Aknwr+fWs6Zy85lTWTYlnIhA79Pu9PfH30hFwDSWpv0PXk0Vg/784Swu/238Gwo4GHedu0MZM45Gr2NL0u+IqNjLeZ9dh39dLs9uy2HbkTKuWTqZ+qZWLn3oU37xZrrLZ5KroaGJYCCqCyHjbWsm8SD2HXAY+L9Dvlz6YSjlTcLfV1Tw/xLyiA7teUKXw+bFlvm/xaupgiVpv4QR1rzXV+JoYXbWY5QFJupKo0MsZ8IaNiU/hG9jEWu2rGdc6XYAxgf5cMtZ0wj39+LprdlsPVKqS1ePApoIBmLHY+BogcXfGNDbtx0t45Oscq7+wJdf7w1kflANv5lxBP+Ggj5foyIokT3Tv01s4XskZj89oDiGu8kn3iGo7hhp076pM4ndoDBiOe8uf44Gr3DO3rGBmUeeQEwrwb52NpwxhYRxgby1J5+bnt5JaY17lkpXzqGJoL+aGyDlcatJaCCb0wO1LR785nAM2ysCuS66kO9NOU6QZ/9XF222+VEWmMiCjD+wYP/vRlUHqkdrI/MO/4XywBnkRZ3t7nDGrBr/WN5d/gx5477IgoP3s3rbDQTU5uBtt3Hd8slcOHcCHx8q5vw/fcI7aQVaOxihNBH0194XoK4Ult0yoLcX1nvwi0OxHKr15Y7441wcVT7wm10RjkxaR713JNNzX8Svvu81iuEuMfsfBNTns2vmD6w5FGrItO9T0P6YfOLfbF5wP1vm/YaQ6kwu/PQKZh75O3ZHIyunRfDmt1YQEeDFzf/cyXWPbyejYGSM/laf09+w/mhthk/+YO0REHdGv9+eVVzD5ZtCKWq0c9e0XFaEDX4/4labNwcnX0OLhw+Jx54hqDpr0Nd0N9/6AmZnPUbeuLMoDF/q7nAUMDX3ZWytDaRP+TrVvtEsOPhHLvnvRUzNfZnESD/e+tZKfnHJLPYdr+TCBz7huy+mcqiwnz/fKU+c+lBDRoeP9kfqs1BxDC68r99t1rtzyrnxyR3YWoX/mX6MKf79a1PtqdmnyR5ERty1zMx+mnO238im5EcoD57Zr+sPG8awJP2XCA52Jf7Q3dGoTprsQRyavJ7A2mwiK/awNO1uyP4b9vlX87UF13LpgrP4y4eZPLsth1d3HWd14jiuXT6ZVQmR2Dy0n2e40gllfdXcAH9dDP4RcNOH/UoEmw4Wces/dxEZ6M3TS3IpLC52SYg+jSVMy3sF7+Yqts77Nbnjz3XJ57hSfN7rLN/3M3bO/BEH464FdPLYcJUVcwUTiz9hWu5LTCz+BA/TSkXANIrCFpEXmMQHFVF8eMKbE3VCVJA3ly2I5oI545k7KRiPXU8O7EOTb+i9jOqSTihzhq1/gcocWPvnPiWB9vHVu3LKeXVXHuODfPjK0lgKi3a5LMQG7wgOxn6FhNwXOGP3d8mLPJPjkavImuzE7TNdKLj6EIvTf01h6CIOTV7v7nBUb0TIH7eK/HGr8GksIS7/bcaXbCX++FtMb32B9i7+xuAIih0BFG/1pHaLF596+hLk7UGon50IP0/8vO1gs1t7evgGg08I+IWDl787/3VjiiaCvqjIhY//ADPXwtQv9uktxhg+OVzCO+kFTI305ytLJ+Njt7k4UGi2B3Ag7nri8/9FdPF/CazL5cS4ldT5TnT5Zw+GV1MlZ+z6Ls32AD5Nug8jrv9eKedp8I4gI/56mm1+5I77In4NBfg2ljItoBFvv3Ci68uJaqilsqqSutpKWutr8Kxvoqm0EU9pwquLvakaPYOo84mi1ncCVf7x1PhOIrM1Z0SsjzXSaCLojcMBb33b+vr8X/fpLU0tDl5PPc6O7HLmTgrmykXReNqGrl/eeHhyZNI6avyiiS18n4s+uYzdM75LZuyVw3IEjmdLHWel3Ip/wwk+XPx/NPhEujskNRjiQZ3vROp8J1IKZI0/ff+MuGMv4R8yjs+KvdhabGdXiQdeLXVESRlJvkXMs+Uwx3aMmKY8JtVkEl38Ma3iSUzRJmg6D6athvHzdH6Jk2gi6M1nD0HWB3DR/RDS+51IRV0TN/9zJzuyyzlrRiTnzIzCwx0/rCIUhSVTETCN8WU7WLL/HqYef43UGXcOq5E49uZKztx5B2GVaWxeeD/FYQvdHZIaAjaBhsoikrwgaRK0ToTsOh/21/iTUT2L12uSqW21aoVxXhVc7reXM21pxDQWwgd3W4+AKJh2Dnh4QsSMkwvkAdqX0E+aCHqStQne/4W1F3Hyjb0W35Fdxp0vpFJU1ciVi6JZEBva63tcrckrhE2L/0Zc/r+Yf+gvrN7+DU5EfIF9026mJCTJrXdUgbXZnLHrTgJrs9mS9DvyolZrx/AI4sz/K5vAVP8Gpvo3cEmUtfxKXoM3+6t9Sa/25+GqFdzvWIWnh3DhFBtXhR5iYeMOfDPehoYKq6YbGgeRMyFyBrS2gE3/vPWVjhrqzvGd8NRaCJkMN2wE35Buiza1OPjT+4d45L9ZRIf68aerk8g40fUYanf8oWvf+tKjtZHpOc8zO+v/8G6upMZnAoXhSygNmk1m3BB2zhoHU/NeZeGB/8Xh4cXmpN9TGLEM0BFCo1FXW6/29/+5xQEHavzY5FjA8Yp6skvrAFgQHci13ps5y5ZKWEU6UpVnvcHuB5MWQcwSiFlqNSMFjh/TTUk6aqi/Dv4bXr7RGip67SvdJgFjDO+mF3LfuxlkFdfy5eRofn7JbAK8PbtNBO7ksHmTEX89h2OuZEna3Ywv287U428Qd+LfRFbuJWf8+ZyIWI7DNvCF9HpkDBOLP2Fu5sOEV6ZRGJbM1nm/pc5Xt0AczZyR3D09YG5QHX6xE1m/JIas4hreTS/kP/sL+V7WfGA+8QEtXDqpiIt904kP88Ijbzts/hOYtuVbfIKtGsO4RIhMhLApkL8bfMNPrT2MwWYlTQQdNVTC+3dbawlNXADrn7fuIjppanHwYUYRf/s4i905FUyN9Ofx65NZPTPKDUH3X6unH0VhyRSFLiKoNpvwyjQmFn9MfP6/aPXwoiRkPoVhiykJmU9F0HQavMIHfidlHIRWHWRS8X+Jy3+boNpsan0msGXeb8ieePGYvkNT/Tc15yW2t618nQwkj4OyEE9SKgNIqQjgr0cm8EczkYgAL1YnXstZyX4s8c4hvCYTig9AUQbsfwPqn+xwVbFu9vwirGGr9eUQFg+h8dZz2x7Oo5lLm4ZEZA3wANZWlY8ZY+7tdF7azl8I1AFfM8b0ONDeJU1DJYdhz3Ow/TForIJlt8LZPz2l86m2sYWdx8rZdLCIN1LzKattYkKwD985J4EvLTx9VFB367QP16aPo9GXElW6jfElW4kq20FoVQaC9bPR4BVGlf9k6nzGU+s7gUavMJo8A2n2DKC1Q+3B1tqAd3MlXs2VBNQdJ6TmMMHVmdhbrWp8YdhisqIv49iENRgPe5dxDNfvjxoZZkaH81GBF/+pT+Sjg8XUNFr7dkeH+rI4sJz5Yc0kBDQz3beSiNYipK4U6kqgtuTz56aaUy/qF/55Umh/Do62HkGTBrUU/VDqqWnIZYlARGzAIeBcIA9rM/v1xpj9HcpcCHwLKxEsBR4wxvQ4pMUZicAc24ojdzsUHUByt+FRfgSDUDtlDblzbqPAfwYVdU3kldVztKSWzOIa0vOraHUY7DbhnJlRXJkczaqEyG6HhY60RNC5HdfeXElo1UFCqw8RUn2IgLpc/OoL8GsowGZaer1egz2EysAEKgKnURY0mxMRX+hyWOhw/X6okS0r9kpaHYYTlfUcK7X2/S4qK6e44fP5KUF2B1MCW5ng20qUr4NxPtYjxKOO8NZiQlqKCGwqwbexGK+GEmz1JUh9+ckbpJP8x7UlhklW0vANbXuEtT2HWH0Wnj5W0vD0Absv2LysEU/i0eEhLqslu6uPYAmQaYw50hbE88A6YH+HMuuAp42VjT4TkRARmWCMOeHsYN5JO8G3n0+l1WH4X9uDXG7bTLEJItWRwBbHdWxsXUrh/jDYX4WVsywTgn2Ij/Dnm6umsHRKOIsmhxLgPfpb1JrtwQTWHqPFw5uS4LmUBM+1ThiDzdGIrbURm6MBD/P58tkOsdFi86PF5ju0nc9KddJ+gzEdwAuYAEuWh1Hc6EFmlY3DVZ4crrJxtMaTzGpPPi3yoKq5/aYuCOi638pOC5OkmAlSxiQpYaKUMqmqlAnVZUw4vpsQqgmmBi/p/7Lyn5NOyaEtQSCw/DY4+yeDuHY3n+jCGsEVwBpjzDfaXl8HLDXG3N6hzL+Ae40xm9tefwD8yBiT0ulaG4ANbS9nAAddEHIEUOKC67qCxuoaIylWGFnxaqyu0Z9YJxtjupyt6cpb267qN52zTl/KYIx5FHjUGUF1R0RSuqs2DTcaq2uMpFhhZMWrsbqGs2J15XoDeUBMh9fRQP4AyiillHIhVyaCHUCCiMSLiBdwNfBmpzJvAl8VyzKg0hX9A0oppbrnsqYhY0yLiNwOvIs1fPTvxph0Ebm57fwjwEasEUOZWMNH3TmTw6VNT06msbrGSIoVRla8GqtrOCXWEbfEhFJKKecafmsSK6WUGlKaCJRSaowb04lARGaISGqHR5WIfMfdcXVHRO4UkXQRSROR50TEx90x9UREvt0Wa/pw+76KyN9FpEhE0jocCxOR90TkcNuz+9cRp9tYr2z7vjpEZNgMdewm1vtEJENE9orIayIS4sYQT9FNvL9qizVVRP4jIsNie7+uYu1w7vsiYkQkYiDXHtOJwBhz0BiTZIxJAhZhdVi/5t6ouiYik4A7gGRjzBysDvir3RtV90RkDnAT1gzz+cDFIpLg3qhO8SSwptOxu4APjDEJwAdtr4eDJzk91jTgcuDjIY+mZ09yeqzvAXOMMfOwlp35f0MdVA+e5PR47zPGzGv7u/Av4OdDHVQ3nuT0WBGRGKylfLpe16YPxnQi6GQ1kGWMOebuQHrgCfiKiCfgx/CeczET+MwYU2eMaQH+C1zm5phOMsZ8DJR1OrwOeKrt66eAS4cypu50Fasx5oAxxhUz7Aelm1j/0/YzAPAZ1nyhYaGbeKs6vPSni0mu7tDNzyzAH4EfMog4NRF87mrgOXcH0R1jzHHg91hZ/wTWnIv/uDeqHqUBq0QkXET8sIYJx/TyHneLap/H0vY8zs3xjEY3Av92dxC9EZFfi0gu8BWGT43gNCKyFjhujNkzmOtoIgDaJrytBYbtUpht7dXrgHhgIuAvIte6N6ruGWMOAL/DahZ4B9gD9L5sqRq1ROQnWD8Dz7g7lt4YY35ijInBivX23sq7Q9sN1k9wQqLSRGC5ANhljCl0dyA9OAc4aowpNsY0A68CX3BzTD0yxjxujFlojFmFVaU97O6YelEoIhMA2p6L3BzPqCEi1wMXA18xI2vy0rPAl9wdRDemYt0Y7hGRbKwmt10i0u8t/zQRWNYzjJuF2uQAy0TEr21Dn9XAATfH1CMRGdf2HIvVsTncv8dvAte3fX098IYbYxk12jao+hGw1hhT5+54etNpUMNaIMNdsfTEGLPPGDPOGBNnjInDWrttoTGmYCAXG9MPrE7XUiDY3bH0Ida7sX4o04B/AN7ujqmXeD/B2n9iD7Da3fF0iu05rL6W5rZfoK8D4VijhQ63PYe5O84eYr2s7etGoBB4191x9hBrJpALpLY9HnF3nL3E+0rb79he4C1gkrvj7C7WTuezgYiBXFuXmFBKqTFOm4aUUmqM00SglFJjnCYCpZQa4zQRKKXUGKeJQCmlxjhNBEopNcZpIlBKqTHu/wN1XpP7K+YDnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y_pred, label='prediction')\n",
    "sns.distplot(y_val, label='target')\n",
    "plt.legend() \n",
    "\n",
    "# sns.displot(y_pred, label='prediction')\n",
    "# sns.displot(y_val, label='target')\n",
    "# plt.legend() \n",
    "\n",
    "# sns.histplot(y_pred, label='prediction')\n",
    "# sns.histplot(y_val, label='target')\n",
    "# plt.legend() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-stress",
   "metadata": {},
   "source": [
    "## 2.4.5 Handling categorical variables\n",
    "\n",
    "\n",
    "We see that adding “age” is quite helpful for the model.\n",
    "\n",
    "\n",
    "Let’s continue adding more features. One of the columns we can use next is the number of\n",
    "doors. This variable appears to be numeric and can take three values: 2, 3, and 4 doors. Even\n",
    "though it’s tempting to put the variable to the model as is, it’s not really a numeric variable:\n",
    "we cannot say that by adding one more door, the price of a car grows (or drops) by a certain\n",
    "amount of money. Rather, the variable is categorical.\n",
    "\n",
    "\n",
    "**Categorical variables describe characteristics of objects and can take one of a few possible\n",
    "values.** The make of a car is a categorical variable, for example; it can be Toyota, BWM, Ford,\n",
    "or any other make. It’s easy to recognize a categorical variable by its values, which typically\n",
    "are strings and not numbers. That’s not always the case, however. The number of doors, for\n",
    "example, is categorical: it can take only one of the three possible values (2, 3, and 4).\n",
    "We can use categorical variables in a machine learning model in multiple ways. One of the\n",
    "simplest ways is to encode such variables by a set of binary features, with a separate feature\n",
    "for each distinct value.\n",
    "\n",
    "\n",
    "In our case, we will create three binary features: num_doors_2, num_doors_3, and\n",
    "num_doors_4. If the car has two doors, num_doors_2 will be set to 1, and the rest will be 0. If\n",
    "the car has three doors, num_doors_3 will get the value 1, and the same goes for\n",
    "num_doors_4. \n",
    "\n",
    "This method of encoding categorical variables is called **one-hot encoding**. We will learn\n",
    "more about this way of encoding categorical variables in chapter 3. For now, let’s choose the simplest way to do this encoding: looping over the possible values (2, 3, and 4) and, for each value, checking whether the value of the observation matches it.\n",
    "Let’s add these lines to the prepare_X function: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "satellite-botswana",
   "metadata": {},
   "source": [
    "   # Iterate over possible values of the “number of doors” variable\n",
    "    for v in [2, 3, 4]: # A \n",
    "        \n",
    "        # Give a feature a meaningful name, such as “num_doors_2” for v=2. \n",
    "        feature = 'num_doors_%s' % v # B\n",
    "        \n",
    "        # Create the one-hot encoding feature. \n",
    "        value = (df['number_of_doors'] == v).astype(int) # C\n",
    "        \n",
    "        # Add the feature back to the dataframe, using the name from B\n",
    "        df[feature] = value #D \n",
    "        features.append(feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-motel",
   "metadata": {},
   "source": [
    "This code may be difficult to understand, so let’s take a closer look at what’s going on here.\n",
    "The most difficult line is C:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "independent-catholic",
   "metadata": {},
   "source": [
    "(df['number_of_doors'] == v).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-picture",
   "metadata": {},
   "source": [
    "Two things happen here. The first one is the expression inside the parentheses, where we use the equals (==) operator. This operation is also an elementwise operation, like the ones we used previously when computing RMSE. In this case, the operation creates a new Pandas series. If elements of the original series equal v, the corresponding elements in the result is True; the elements are False otherwise. The operation creates a series of True/False values. Because v has three values (2, 3, and 4), and we apply this operation to every value of v.\n",
    "\n",
    "Next, we convert the Boolean series to integers in such a way that True becomes 1 and False\n",
    "becomes 0, which is easy to do with the astype(int) method (figure 2.25). Now we can use\n",
    "the results as features and put them into linear regression.\n",
    "\n",
    "The number of doors, as we discussed, is a categorical variable that appears to be numerical\n",
    "because the values are integers (2, 3 and 4). All the remaining categorical variables we have\n",
    "in the dataset are strings.\n",
    "We can use the same approach to encode other categorical variables. Let’s start with\n",
    "make. For our purposes, it should be enough to get and use only the most frequently\n",
    "occurring values. Let’s find out what the five most frequent values are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sustained-monaco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chevrolet     1123\n",
       "ford           881\n",
       "volkswagen     809\n",
       "toyota         746\n",
       "dodge          626\n",
       "Name: make, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['make'].value_counts().head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-delivery",
   "metadata": {},
   "source": [
    "We take these values and use them to encode make in the same way that we encoded the number of doors. We create five new variables called is_make_chevrolet, is_make_ford, is_make_volkswagen, is_make_toyota, and is_make_dodge: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "known-party",
   "metadata": {},
   "source": [
    "for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n",
    "    feature = 'is_make_%s' % v\n",
    "    df[feature] = (df['make'] == v).astype(int)\n",
    "    features.append(feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-somalia",
   "metadata": {},
   "source": [
    "Now the whole prepare_X should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "magnetic-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df):\n",
    "    # create a copy of the input parameter to prevent side effects\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create a copy of the base list with the basic features\n",
    "    features = base.copy()\n",
    "    \n",
    "    # compute the age feature\n",
    "    df['age'] = 2017 - df.year\n",
    "    \n",
    "    # append age to the list of feature names we use for the model\n",
    "    features.append('age')\n",
    "    \n",
    "    # encode the “number of doors” variable\n",
    "    # Iterate over possible values of the “number of doors” variable\n",
    "    for v in [2, 3, 4]: # A \n",
    "        \n",
    "        # Give a feature a meaningful name, such as “num_doors_2” for v=2. \n",
    "        feature = 'num_doors_%s' % v # B\n",
    "        \n",
    "        # Create the one-hot encoding feature. \n",
    "        value = (df['number_of_doors'] == v).astype(int) # C\n",
    "        \n",
    "        # Add the feature back to the dataframe, using the name from B\n",
    "        df[feature] = value #D \n",
    "        features.append(feature) \n",
    "        \n",
    "    # encode the “make” variable\n",
    "    # Iterate over possible values of the “make” variable\n",
    "    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n",
    "        \n",
    "         # Give a feature a meaningful name, such as 'is_make_toyota' for v='toyota'.\n",
    "        feature = 'is_make_%s' % v\n",
    "        \n",
    "        # Add the feature back to the dataframe, using the feature variable\n",
    "        df[feature] = (df['make'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "        \n",
    "    # declare a variable named, df_num. set the value to the base columns of df dataframe.\n",
    "    df_num = df[features]\n",
    "    \n",
    "    # replace Nan with zeros\n",
    "    df_num = df_num.fillna(0)\n",
    "    \n",
    "    # declare a variable named, X. set the value to df_num.values.\n",
    "    X = df_num.values\n",
    "    \n",
    "    # return X\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-newspaper",
   "metadata": {},
   "source": [
    "Let’s check whether this code improves the RMSE of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "hawaiian-figure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.45701060e-03,  9.17408538e-02, -6.18372418e-03,  1.23571673e-02,\n",
       "       -2.89616609e-05, -9.51811382e-02, -1.25096146e+00, -1.46396979e+00,\n",
       "       -1.29539677e+00, -1.95721851e-01, -7.92361383e-02,  2.88307619e-02,\n",
       "       -2.87389672e-01, -2.83585754e-01])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the training data\n",
    "X_train = prepare_X(df_train)\n",
    "\n",
    "# train the model using training data\n",
    "w_0, w = linear_regression(X_train, y_train)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "constant-grounds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.5076038849557034\n"
     ]
    }
   ],
   "source": [
    "# prepare the validation data\n",
    "X_val = prepare_X(df_val)\n",
    "\n",
    "# apply the model to the validation dataset.\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "# print the rmse\n",
    "print('validation:', rmse(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-blink",
   "metadata": {},
   "source": [
    "The previous value was 0.517, so we managed to improve the RMSE score further.\n",
    "We can use a few more variables: “engine_fuel_type”, “transmission_type”,\n",
    "“driven_wheels”, “market_category”, “vehicle_size”, and “vehicle_style”. Let’s do the same\n",
    "thing for them. After the modifications, the prepare_X starts looking a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aware-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df):\n",
    "    # create a copy of the input parameter to prevent side effects\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create a copy of the base list with the basic features\n",
    "    features = base.copy()\n",
    "    \n",
    "    # compute the age feature\n",
    "    df['age'] = 2017 - df.year\n",
    "    \n",
    "    # append age to the list of feature names we use for the model\n",
    "    features.append('age')\n",
    "    \n",
    "    # encode the “number of doors” variable\n",
    "    # Iterate over possible values of the “number of doors” variable\n",
    "    for v in [2, 3, 4]: # A \n",
    "        \n",
    "        # Give a feature a meaningful name, such as “num_doors_2” for v=2. \n",
    "        feature = 'num_doors_%s' % v # B\n",
    "        \n",
    "        # Create the one-hot encoding feature. \n",
    "        value = (df['number_of_doors'] == v).astype(int) # C\n",
    "        \n",
    "        # Add the feature back to the dataframe, using the name from B\n",
    "        df[feature] = value #D \n",
    "        features.append(feature) \n",
    "        \n",
    "    # encode the “make” variable\n",
    "    # Iterate over possible values of the “make” variable\n",
    "    for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']:\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_make_toyota' for v='toyota'.\n",
    "        feature = 'is_make_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['make'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "        \n",
    "        \n",
    "    #\n",
    "    # encode the “engine_fuel_type” variable\n",
    "    # Iterate over possible values of the “engine_fuel_type” variable\n",
    "    for v in ['regular_unleaded', 'premium_unleaded_(required)','premium_unleaded_(recommended)','flex-fuel_(unleaded/e85)']: #A\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_type_premium_unleaded_required' for v='premium_unleaded_required'.\n",
    "        feature = 'is_type_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['engine_fuel_type'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # encode the “transmission_type” variable\n",
    "    # Iterate over possible values of the “transmission_type” variable\n",
    "    for v in ['automatic', 'manual', 'automated_manual']: #B\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_transmission_manual' for v='manual'.\n",
    "        feature = 'is_transmission_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['transmission_type'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # encode the “driven_wheels” variable\n",
    "    # Iterate over possible values of the “driven_wheels” variable\n",
    "    for v in ['front_wheel_drive', 'rear_wheel_drive','all_wheel_drive', 'four_wheel_drive']: #C\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_driven_wheels_rear_wheel_drive' for v='rear_wheel_drive'.\n",
    "        feature = 'is_driven_wheels_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['driven_wheels'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # encode the “market_category” variable\n",
    "    # Iterate over possible values of the “market_category” variable\n",
    "    for v in ['crossover', 'flex_fuel', 'luxury','luxury,performance', 'hatchback']: #D\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_mc_luxury' for v='luxury'.\n",
    "        feature = 'is_mc_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['market_category'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # encode the “vehicle_size” variable\n",
    "    # Iterate over possible values of the “vehicle_size” variable\n",
    "    for v in ['compact', 'midsize', 'large']: #E\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_size_midsize' for v='midsize'.\n",
    "        feature = 'is_size_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['vehicle_size'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    # encode the “vehicle_style” variable\n",
    "    # Iterate over possible values of the “vehicle_style” variable\n",
    "    for v in ['sedan', '4dr_suv', 'coupe', 'convertible','4dr_hatchback']: #F\n",
    "        \n",
    "        # Give a feature a meaningful name, such as 'is_style_coupe' for v='coupe'.\n",
    "        feature = 'is_style_%s' % v\n",
    "        \n",
    "        # Add the one-hot encoding feature to the dataframe, using the feature variable\n",
    "        df[feature] = (df['vehicle_style'] == v).astype(int)\n",
    "        features.append(feature)\n",
    "    \n",
    "    #\n",
    "    \n",
    "    # declare a variable named, df_num. set the value to the base columns of df dataframe.\n",
    "    df_num = df[features]\n",
    "    \n",
    "    # replace Nan with zeros\n",
    "    df_num = df_num.fillna(0)\n",
    "    \n",
    "    # declare a variable named, X. set the value to df_num.values.\n",
    "    X = df_num.values\n",
    "    \n",
    "    # return X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "annoying-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 830.8920785817741\n"
     ]
    }
   ],
   "source": [
    "# prepare the training data\n",
    "X_train = prepare_X(df_train)\n",
    "\n",
    "# train the model using training data\n",
    "w_0, w = linear_regression(X_train, y_train)\n",
    "\n",
    "# prepare the validation data\n",
    "X_val = prepare_X(df_val) \n",
    "\n",
    "# apply the model to X_val \n",
    "y_pred = w_0 + X_val.dot(w) \n",
    "\n",
    "# print the rmse\n",
    "print('validation:', rmse(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-father",
   "metadata": {},
   "source": [
    "The number we see is significantly worse than before. We get 34.2, which is a lot more than\n",
    "the 0.5 we had before.\n",
    "NOTE: The number you get may be different, depending on the Python version, NumPy version, the versions\n",
    "of NumPy dependencies, OS, and other factors. But the jump in the validation metric from 0.5 to something\n",
    "significantly bigger should always alert us.\n",
    "Instead of helping, the new features made the score a lot worse. Luckily, we have validation to\n",
    "help us spot this problem. In the next section, we will see why it happens and how to deal\n",
    "with it.\n",
    "\n",
    "## 2.4.6 Regularization\n",
    "\n",
    "\n",
    "We saw that adding new features does not always help, and in our case, it made things a lot\n",
    "worse. The reason for this behavior is numerical instability. Recall the formula of the normal\n",
    "equation:\n",
    "\n",
    "\n",
    "One of the terms in the equation is the inverse of the XTX matrix:\n",
    "The inversion is the issue in our case. Sometimes, when adding new columns to X, we can\n",
    "accidentally add a column that is a combination of other columns. For example, if we already\n",
    "have the mpg in the city feature and decide to add kilometers per liter in the city, the second\n",
    "feature is the same as the first one but multiplied by a constant.\n",
    "\n",
    "\n",
    "When this happens, XTX becomes undetermined or singular, which means that it’s not\n",
    "possible to find an inverse for this matrix. If we try to invert a singular matrix, NumPy will tell\n",
    "us about that by raising a LinAlgError:\n",
    "\n",
    "Our code didn’t raise any exceptions, however. It happened because we don’t typically have\n",
    "columns that are perfect linear combinations of other columns. The real data is often noisy,\n",
    "with measurement errors (such as recording 1.3 instead of 13 for mpg), rounding errors (such\n",
    "as storing 0.0999999 instead of 0.1), and many other errors. Technically, such matrices are\n",
    "not singular, so NumPy doesn’t complain.\n",
    "\n",
    "\n",
    "For this reason, however, some of the values in the weights become extremely large — a\n",
    "lot larger than they are supposed to be.\n",
    "\n",
    "\n",
    "If we look at the values of our w0 and w, we see that this is indeed the case. The bias term\n",
    "w0 has the value 5788519290303866.0, for example (the value may vary depending on the\n",
    "machine, OS, and version of NumPy), and a few components of w have extremely large\n",
    "negative values as well.\n",
    "\n",
    "\n",
    "In numerical linear algebra, such issues are called numerical instability issues, and they\n",
    "are typically solved with regularization techniques. The aim of regularization is to make sure\n",
    "that the inverse exists by forcing the matrix to be invertible. Regularization is an important\n",
    "concept in machine learning: it means “controlling” — controlling the weights of the model so\n",
    "that they behave correctly and don’t grow too large, as in our case.\n",
    "\n",
    "One way to do regularization is to add a small number to each diagonal element of the\n",
    "matrix. Then we get the following formula for linear regression:\n",
    "\n",
    "\n",
    "NOTE: Regularized linear regression is often called ridge regression. Many libraries, including scikit-learn, use\n",
    "ridge to refer to regularized linear regression and linear regression to refer to the unregularized model.\n",
    "\n",
    "\n",
    "Let’s look at the part that changed: the matrix that we need to invert. This is how it looks:\n",
    "This formula says that we need I — an identity matrix, which is a matrix with ones on the\n",
    "main diagonal and zeros everywhere else. We multiply this identity matrix by a number α. This\n",
    "way, all the ones on the diagonal of I become α. Then we sum αI and XTX, which adds α to all\n",
    "the diagonal elements of XTX.\n",
    "\n",
    "\n",
    "This formula can directly translate to NumPy code:\n",
    "\n",
    "XTX = X_train.T.dot(X_train)\n",
    "\n",
    "XTX = XTX + 0.01 * np.eye(XTX.shape[0])\n",
    "\n",
    "\n",
    "The np.eye function creates a two-dimensional NumPy array that is also an identity matrix.\n",
    "When we multiply by 0.01, the ones on the diagonal become 0.01, so when we add this matrix\n",
    "to XTX, we add only 0.01 to its main diagonal (figure 2.26). \n",
    "\n",
    "Let’s create a new function that uses this idea and implements linear regression with\n",
    "regularization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "centered-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the amount of regularization by using the parameter r.\n",
    "def linear_regression_reg(X, y, r=0.0): #A\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "    XTX = X.T.dot(X)\n",
    "    \n",
    "    # Add r to the main diagonal of XTX. \n",
    "    reg = r * np.eye(XTX.shape[0]) #B\n",
    "    XTX = XTX + reg #B\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    w = XTX_inv.dot(X.T).dot(y)\n",
    "    return w[0], w[1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-program",
   "metadata": {},
   "source": [
    "The function is very similar to linear regression, but a few lines are different. First, there’s an\n",
    "extra parameter r that controls the amount of regularization — this corresponds to the\n",
    "number α in the formula that we add to the main diagonal of XTX.\n",
    "\n",
    "\n",
    "Regularization affects the final solution by making the components of w smaller. We can\n",
    "see that the more regularization we add, the smaller the weights become.\n",
    "\n",
    "\n",
    "Let’s check what happens with our weights for different values of r:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "latin-making",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0, -694700695602717184.00, 278.96, 694700695602667008.00\n",
      "0.001, 7.19, -0.10, 1.81\n",
      " 0.01, 7.18, -0.10, 1.81\n",
      "  0.1, 7.05, -0.10, 1.78\n",
      "    1, 6.22, -0.10, 1.56\n",
      "   10, 4.39, -0.09, 1.08\n"
     ]
    }
   ],
   "source": [
    "for r in [0, 0.001, 0.01, 0.1, 1, 10]:\n",
    "    w_0, w = linear_regression_reg(X_train, y_train, r=r)\n",
    "    print('%5s, %.2f, %.2f, %.2f' % (r, w_0, w[13], w[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-hypothesis",
   "metadata": {},
   "source": [
    "We start with 0, which is an unregularized solution, and get very large numbers. Then we try\n",
    "0.001 and increase it by 10 times on each step: 0.01, 0.1, 1, and 10. We see that the values\n",
    "that we selected become smaller as r grows.\n",
    "\n",
    "\n",
    "Now let’s check whether regularization helps with our problem and what RMSE we get after\n",
    "that. Let’s run it with r=0.001: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "foreign-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.46022676266043516\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w_0, w = linear_regression_reg(X_train, y_train, r=0.001)\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "print('validation:', rmse(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-bundle",
   "metadata": {},
   "source": [
    "This result is an improvement over the previous score: 0.507\n",
    "\n",
    "NOTE: Sometimes, when adding a new feature causes performance degradation, simply removing this\n",
    "feature may be enough to solve the problem. Having a validation dataset is important to decide whether to\n",
    "add regularization, remove the feature, or do both: we use the score on the validation data to choose the best\n",
    "option. In our particular case we see that adding regularization helps: it improves the score we had previously.\n",
    "\n",
    "\n",
    "We tried using r=0.001, but we should try other values as well. Let’s run a grid search to\n",
    "select the best parameter r: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adjustable-bidder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1e-06 0.4602255729429436\n",
      "0.0001 0.4602254945347706\n",
      " 0.001 0.46022676266043516\n",
      "  0.01 0.46023949632611183\n",
      "   0.1 0.46037006958137333\n",
      "     1 0.4618298042653896\n",
      "     5 0.4684079627533808\n",
      "    10 0.4757248100693528\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "X_val = prepare_X(df_val)\n",
    "for r in [0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:\n",
    "    w_0, w = linear_regression_reg(X_train, y_train, r=r)\n",
    "    y_pred = w_0 + X_val.dot(w)\n",
    "    print('%6s' %r, rmse(y_val, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-guarantee",
   "metadata": {},
   "source": [
    "We see that the best performance is achieved with a smaller r: \n",
    "    \n",
    "We also notice that the performance for values below 0.1 don’t change much except in the\n",
    "sixth digit, which we shouldn’t consider to be significant.\n",
    "\n",
    "\n",
    "Let’s take the model with r=0.01 as the final model. Now we can check it against the test\n",
    "dataset to verify if the model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "danish-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: 0.46023949632611183\n",
      "test: 0.4571813679692604\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w_0, w = linear_regression_reg(X_train, y_train, r=0.01)\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "print('validation:', rmse(y_val, y_pred))\n",
    "X_test = prepare_X(df_test)\n",
    "y_pred = w_0 + X_test.dot(w)\n",
    "print('test:', rmse(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-miracle",
   "metadata": {},
   "source": [
    "Because these two numbers are pretty close, we conclude that the model can generalize well\n",
    "to the new unseen data. \n",
    "\n",
    "## Exercise 2.4\n",
    "\n",
    "\n",
    "Regularization is needed because\n",
    "\n",
    "\n",
    "a) It can control the weights of the model and not let them grow too large\n",
    "b) The real-world data is noisy\n",
    "c) We often have numerical instability problems\n",
    "\n",
    "\n",
    "## 2.4.7 (Multiple answers are possible)Using the model\n",
    "\n",
    "\n",
    "As we now have a model, we can start using it for predicting the price of a car.\n",
    "Suppose that a user posts the following ad on our website: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fallen-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = {\n",
    "    'city_mpg': 18,\n",
    "    'driven_wheels': 'all_wheel_drive',\n",
    "    'engine_cylinders': 6.0,\n",
    "    'engine_fuel_type': 'regular_unleaded',\n",
    "    'engine_hp': 268.0,\n",
    "    'highway_mpg': 25,\n",
    "    'make': 'toyota',\n",
    "    'market_category': 'crossover,performance',\n",
    "    'model': 'venza',\n",
    "    'number_of_doors': 4.0,\n",
    "    'popularity': 2031,\n",
    "    'transmission_type': 'automatic',\n",
    "    'vehicle_size': 'midsize',\n",
    "    'vehicle_style': 'wagon',\n",
    "    'year': 2013\n",
    "} \n",
    "\n",
    "ad2 = {    \n",
    "    'city_mpg': 22,\n",
    "    'driven_wheels': 'all_wheel_drive',\n",
    "    'engine_cylinders': 4.0,\n",
    "    'engine_fuel_type': 'regular_unleaded',\n",
    "    'engine_hp': 168,\n",
    "    'highway_mpg': 30,\n",
    "    'make': 'ford',\n",
    "    'market_category': 'crossover',\n",
    "    'model': 'escape',\n",
    "    'number_of_doors': 4.0,\n",
    "    'popularity': 5657,\n",
    "    'transmission_type': 'automatic',\n",
    "    'vehicle_size': 'compact',\n",
    "    'vehicle_style': '4dr_suv',\n",
    "    'year': 2008\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-pepper",
   "metadata": {},
   "source": [
    "We’d like to suggest the price for this car. For that, we use our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "offensive-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame([ad2])\n",
    "X_test = prepare_X(df_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-uniform",
   "metadata": {},
   "source": [
    "First, we create a small dataframe with one row. This row contains all the values of the ad\n",
    "dictionary we created earlier. Next, we convert this dataframe to a matrix.\n",
    "Now we can apply our model to the matrix to predict the price of this car:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "activated-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w_0 + X_test.dot(w) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-baker",
   "metadata": {},
   "source": [
    "This prediction is not the final price, however; it's the logarithm of the price. To get the actual\n",
    "price, we need to undo the logarithm and apply the exponent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "authentic-universal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14131.94986182])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suggestion = np.expm1(y_pred)\n",
    "\n",
    "# OR\n",
    "\n",
    "suggestion = np.exp(y_pred)\n",
    "suggestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-adelaide",
   "metadata": {},
   "source": [
    "The output is 28,294.13. The real price of this car is $31,120, so our model is not far from the\n",
    "actual price. \n",
    "\n",
    "## 2.5 Next steps\n",
    "\n",
    "\n",
    "## 2.5.1 Exercises\n",
    "\n",
    "\n",
    "There are a few other things you can try to make the model better. For example,\n",
    "\n",
    "\n",
    "* Try more feature engineering. When implementing category encoding, we included only\n",
    "the top five values for each categorical variable. Including more values during the\n",
    "encoding process might improve the model. Try doing that, and re-evaluate the quality\n",
    "of the model in terms of RMSE.\n",
    "\n",
    "\n",
    "* Write a function for binary encoding. In this chapter we implemented the category\n",
    "encoding manually: we looked at the top five values, wrote them in a list, and then\n",
    "looped over the list to create binary features. Doing it this way is cumbersome, which is\n",
    "why it’s a good idea to write a function that will do this automatically. It should have\n",
    "multiple arguments: the dataframe, the name of the categorical variable and the\n",
    "number of most frequent values it should consider. This function should also help us do\n",
    "the previous exercise.\n",
    "\n",
    "\n",
    "## 2.5.2 Other projects\n",
    "\n",
    "\n",
    "There are other projects you can do now:\n",
    "\n",
    "\n",
    "* Predict the price of a house. You can take the Boston house prices dataset from\n",
    "https://www.kaggle.com/vikrishnan/boston-house-prices or https://scikitlearn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "\n",
    "\n",
    "* Check other datasets, such as https://archive.ics.uci.edu/ml/datasets.php?task=reg,\n",
    "that have numerical target values. For example, we can use the data from the student\n",
    "performance dataset (http://archive.ics.uci.edu/ml/datasets/Student+Performance) to\n",
    "train a model for determining the performance of students.\n",
    "\n",
    "\n",
    "## 2.6 Summary\n",
    "\n",
    "\n",
    "* Doing simple initial exploratory analysis is important. Among other things, it helps us\n",
    "find out whether the data has missing values. It’s not possible to train a linear\n",
    "regression model when there are missing values, so it’s important to check our data\n",
    "and fill in the missing values if necessary.\n",
    "\n",
    "\n",
    "* As a part of exploratory data analysis, we need to check the distribution of the target\n",
    "variable. If the target distribution has a long tail, we need to apply the log\n",
    "transformation. Without it, we may get inaccurate and misleading predictions from the\n",
    "linear regression model.\n",
    "\n",
    "\n",
    "* The train/validation/test split is the best way to check our models. It gives us a way to\n",
    "measure the performance of the model reliably, and things like numerical instability issues won’t go unnoticed.\n",
    "\n",
    "\n",
    "* The linear regression model is based on a simple mathematical formula, and\n",
    "understanding this formula is the key to successful application of the model. Knowing\n",
    "these details helps us learn how the model works before coding it.\n",
    "\n",
    "\n",
    "* It’s not difficult to implement linear regression from scratch, using Python and NumPy.\n",
    "Doing so helps us understand that there's no magic behind machine learning: it’s\n",
    "simple math translated to code.\n",
    "\n",
    "\n",
    "* RMSE gives us a way to measure the predictive performance of our model on the\n",
    "validation set. It lets us confirm that the model is good and helps us compare multiple\n",
    "models to find the best one.\n",
    "\n",
    "\n",
    "* Feature engineering is the process of creating new features. Adding new features is\n",
    "important for improving the performance of a model. While adding new features, we\n",
    "always need to use the validation set to make sure that our model indeed improves.\n",
    "Without constant monitoring, we risk getting mediocre or very bad performance.\n",
    "\n",
    "\n",
    "* Sometimes, we face numerical instability issues that we can solve with regularization.\n",
    "Having a good way to validate models is crucial for spotting a problem before it’s too\n",
    "late.\n",
    "\n",
    "\n",
    "* After the model is trained and validated, we can use it to make predictions, such as\n",
    "applying it to cars with unknown prices to estimate how much they may cost.\n",
    "In chapter 3, we will learn how to do classification with machine learning, using logistic\n",
    "regression to predict customer churn.\n",
    "\n",
    "\n",
    "## 2.7 Answers to exercises\n",
    "\n",
    "\n",
    "* Exercise 2.1 B) Values spread far from the head\n",
    "* Exercise 2.2 A) xi is a feature vector and yi is the logarithm of the price\n",
    "* Exercise 2.3 B) A vector y with price predictions\n",
    "* Exercise 2.4 A), B) and C) All three answers are correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "inner-canvas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>year</th>\n",
       "      <th>engine_fuel_type</th>\n",
       "      <th>engine_hp</th>\n",
       "      <th>engine_cylinders</th>\n",
       "      <th>transmission_type</th>\n",
       "      <th>driven_wheels</th>\n",
       "      <th>number_of_doors</th>\n",
       "      <th>market_category</th>\n",
       "      <th>vehicle_size</th>\n",
       "      <th>vehicle_style</th>\n",
       "      <th>highway_mpg</th>\n",
       "      <th>city_mpg</th>\n",
       "      <th>popularity</th>\n",
       "      <th>msrp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>ford</td>\n",
       "      <td>escape</td>\n",
       "      <td>2015</td>\n",
       "      <td>premium_unleaded_(recommended)</td>\n",
       "      <td>178.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>front_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>crossover</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_suv</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>5657</td>\n",
       "      <td>29735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>ford</td>\n",
       "      <td>escape</td>\n",
       "      <td>2015</td>\n",
       "      <td>premium_unleaded_(recommended)</td>\n",
       "      <td>178.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>front_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>crossover</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_suv</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>5657</td>\n",
       "      <td>25650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>ford</td>\n",
       "      <td>escape</td>\n",
       "      <td>2015</td>\n",
       "      <td>premium_unleaded_(recommended)</td>\n",
       "      <td>178.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>all_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>crossover</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_suv</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>5657</td>\n",
       "      <td>27400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>ford</td>\n",
       "      <td>escape</td>\n",
       "      <td>2015</td>\n",
       "      <td>regular_unleaded</td>\n",
       "      <td>168.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>front_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>crossover</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_suv</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>5657</td>\n",
       "      <td>23450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>ford</td>\n",
       "      <td>escape</td>\n",
       "      <td>2015</td>\n",
       "      <td>premium_unleaded_(recommended)</td>\n",
       "      <td>178.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>automatic</td>\n",
       "      <td>all_wheel_drive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>crossover</td>\n",
       "      <td>compact</td>\n",
       "      <td>4dr_suv</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>5657</td>\n",
       "      <td>31485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      make   model  year                engine_fuel_type  engine_hp  \\\n",
       "4192  ford  escape  2015  premium_unleaded_(recommended)      178.0   \n",
       "4193  ford  escape  2015  premium_unleaded_(recommended)      178.0   \n",
       "4194  ford  escape  2015  premium_unleaded_(recommended)      178.0   \n",
       "4195  ford  escape  2015                regular_unleaded      168.0   \n",
       "4196  ford  escape  2015  premium_unleaded_(recommended)      178.0   \n",
       "\n",
       "      engine_cylinders transmission_type      driven_wheels  number_of_doors  \\\n",
       "4192               4.0         automatic  front_wheel_drive              4.0   \n",
       "4193               4.0         automatic  front_wheel_drive              4.0   \n",
       "4194               4.0         automatic    all_wheel_drive              4.0   \n",
       "4195               4.0         automatic  front_wheel_drive              4.0   \n",
       "4196               4.0         automatic    all_wheel_drive              4.0   \n",
       "\n",
       "     market_category vehicle_size vehicle_style  highway_mpg  city_mpg  \\\n",
       "4192       crossover      compact       4dr_suv           32        23   \n",
       "4193       crossover      compact       4dr_suv           32        23   \n",
       "4194       crossover      compact       4dr_suv           30        22   \n",
       "4195       crossover      compact       4dr_suv           31        22   \n",
       "4196       crossover      compact       4dr_suv           30        22   \n",
       "\n",
       "      popularity   msrp  \n",
       "4192        5657  29735  \n",
       "4193        5657  25650  \n",
       "4194        5657  27400  \n",
       "4195        5657  23450  \n",
       "4196        5657  31485  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df[df['make'] =='ford']\n",
    "x = x[x['model'] == 'escape']\n",
    "x = x[x['year'] == 2015]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-worse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvizenv]",
   "language": "python",
   "name": "conda-env-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
